{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Regression de Ridge (L2) par methode des moindres carres permettant la penalisation du surapprentissage.\n",
    "\n",
    "Arguments :\n",
    "\n",
    "    y : Variable d'interet (dimension : n x 1)\n",
    "    x : matrice contenant nos variables explicatives (dimension : n x p)\n",
    "    l : facteur qui permet de diminuer l'amplitude des coefficients de regression\n",
    "    minY : borne inférieur appliquer sur les predictions\n",
    "    maxY : borne supérieur appliquer sur les predictions\n",
    "    \n",
    "    ou,\n",
    "        n : nombre d'observation\n",
    "        p : nombre de variable explicative       \n",
    "        \n",
    "\n",
    "Retourne : \n",
    "\n",
    "    B = Matrices de nos coefficiants de regression (dimension (p+1) x 1)\n",
    "    Y_estime = Nouvelles predictions avec les coefficiants B\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ridge_regression(y,x,l,minY,maxY):\n",
    "    \n",
    "    # Nombre d'observations\n",
    "    n = len(x)\n",
    "    \n",
    "    # Nombre de variable explicative\n",
    "    p = len(x.T)\n",
    "    \n",
    "    # Colonne de 1\n",
    "    Ones = np.array([np.ones(n)]).T\n",
    "    \n",
    "    # On l'emboite a notre matrice x\n",
    "    x_withOne = np.concatenate((Ones,x),axis = 1)    \n",
    "    \n",
    "    # Matrice identite de taille p+1\n",
    "    identi = np.identity(p+1) \n",
    "    identi[0][0] = 0 # on met un zero vis a vis le coeff b0 qui est l'ordonnee a l'origine\n",
    "    identi = np.dot(l,identi)\n",
    "    \n",
    "    # On multiplie X par soi meme\n",
    "    x2_withOne = np.dot(x_withOne.T,x_withOne)\n",
    "    \n",
    "    # On calcule la matrice de variance-covariance avec un biais de penalite\n",
    "    C = np.linalg.inv(x2_withOne + identi)\n",
    "    \n",
    "    # On calcule les coefficients de regression\n",
    "    B = np.dot(y,np.dot(x_withOne,C))\n",
    "    \n",
    "    # On calcule les predictions\n",
    "    Y_estime = np.dot(x_withOne,B)\n",
    "    \n",
    "    # On borne nos prediction\n",
    "    Y_estime_borne = Y_estime.clip(minY,maxY)\n",
    "    \n",
    "    return B, Y_estime_borne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Fonction permettant de transformer une matrice pour que ses colonnes aient une moyenne = 0 et une variance = 1\n",
    "\n",
    "Argument:\n",
    "    dataset : le jeu de donnees a transformer\n",
    "    \n",
    "Retourne :\n",
    "    dataset_std : le jeu de donnees transforme\n",
    "\n",
    "\"\"\"\n",
    "def scaler(dataset):\n",
    "    \n",
    "    # On calcule la moyenne pour chaque colonne\n",
    "    u = np.array([np.mean(dataset, axis=0)])\n",
    "    \n",
    "    # On soustrait les colonnes du jeu de donnees par ses moyennes\n",
    "    dataset_std = dataset - u\n",
    "    \n",
    "    # On calcule la variance pour chaque colonne\n",
    "    sigma = np.array([np.std(dataset_std, axis=0)])\n",
    "    \n",
    "    # On divise les colonnes du jeu de donnees par ses variances\n",
    "    dataset_std = dataset_std/sigma\n",
    "    \n",
    "    # Si contient des NaN cause par une division par 0 (si la variance etait de 0)\n",
    "    if(np.isnan(dataset_std).any()):\n",
    "        print(\"Colonne de variance = 0, contient des NaN\") # On affiche le message d'erreur\n",
    "        \n",
    "    \n",
    "    return dataset_std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "L'algorithme R2_prev implemente une validation croise sur l'ensemble du jeu de donnees fournis\n",
    "\n",
    "Arguments:\n",
    "    Y : La variable d'interet\n",
    "    W : Notre estimation de la variable d'interet\n",
    "    X : Nos variables explicatives\n",
    "    \n",
    "\n",
    "\n",
    "Retourne:\n",
    "\n",
    "    R2_prev : Une mesure bornee entre [-inf, 1] qui represente le pouvoir predictif de nos estimations\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def R2_prev(Y,W,X):\n",
    "    \n",
    "    # On multiplie X par soi meme\n",
    "    X2 = np.dot(X.T,X)  \n",
    "    \n",
    "    # The variance-covariance\n",
    "    C = np.linalg.inv(X2)\n",
    "    \n",
    "    # Hat Matrix (X*B = H*Y)\n",
    "    H = np.dot(np.dot(X,C),X.T)\n",
    "    \n",
    "    # We are only interested in the (i,i) values\n",
    "    H = np.diagonal(H)\n",
    "    \n",
    "    # Mean of Y\n",
    "    y_S = np.sum(Y)/float(len(Y))\n",
    "    \n",
    "    num = 0.0\n",
    "    for i in range(len(Y)):\n",
    "        num += ((Y[i] - W[i])/float(1 - H[i]))**2\n",
    "\n",
    "    denom = 0.0\n",
    "    for i in range(len(Y)):\n",
    "        denom += (Y[i] - y_S)**2\n",
    "    \n",
    "    R2_prev = (1 - num/denom)\n",
    "    \n",
    "    return R2_prev\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Selon le document MTH3302_CriteresProjet.pdf nous sommes evalues par notre root mean squared error (RMSE).\n",
    "On trouve ici son implementation.\n",
    "\n",
    "Arguments :\n",
    "    Y : La variable d'interet\n",
    "    W : Notre estimation de la variable d'interet\n",
    "    \n",
    "    \n",
    "Retourne:\n",
    "    rmse : L'erreur moyenne de nos predictions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def RMSE(Y,W):\n",
    "\n",
    "    n = len(Y)\n",
    "\n",
    "    total = 0.0\n",
    "    for i in range(n):\n",
    "        total += (Y[i] - W[i])**2\n",
    "        \n",
    "    rmse = total/float(n)\n",
    "        \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--- Variable Qualitative -> Table Binaire --- \n",
    "\n",
    "Afin de traiter nos variables qualitatives, nous avons une fonction qui la tranforme en table binaire \n",
    "avec un one bit encoder. \n",
    "\n",
    "    Disons que la variable explicative peut prendre 4 valeurs {Bleu, Vert, Jaune, Rouge},\n",
    "        \n",
    "          ID        X1\n",
    "        ----------------\n",
    "          1        Bleu\n",
    "          2        Bleu\n",
    "          3        Vert\n",
    "          4        Jaune\n",
    "          5        Vert\n",
    "          6        Rouge          \n",
    "         ...\n",
    "    \n",
    "    Suite a un encodage en 1 bit notre variable explicative X1 devient,\n",
    "    \n",
    "          ID        Bleu        Vert        Jaune        Rouge\n",
    "        -------------------------------------------------------\n",
    "          1          1           0            0            0\n",
    "          2          1           0            0            0\n",
    "          3          0           1            0            0\n",
    "          4          0           0            1            0\n",
    "          5          0           1            0            0\n",
    "          6          0           0            0            1\n",
    "                                ...\n",
    "                                \n",
    "\n",
    "Nous nous basons sur la fonction get_dummies() de la librairie Panda pour faire ceci.\n",
    "\n",
    "\n",
    "Arguments,\n",
    "\n",
    "    trainSet : le jeu de donnees de training\n",
    "    testSet : le jeu de donnees de test\n",
    "    varName : la variable qualitative que nous souhaitons transformer en tableau binaire\n",
    "    \n",
    "    \n",
    "Retourne, \n",
    "    trainSet_new : le jeu de donnees de training ou la variable qualitative a ete remplacee par un tableau binaire\n",
    "    testSet_new : le jeu de donnees de test ou la variable qualitative a ete remplacee par un tableau binaire\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def qualiToBinary(trainSet, testSet, varName):\n",
    "    \n",
    "    # Genere la table binaire de la variable qualitative\n",
    "    trainVar_binary = pd.get_dummies(trainSet[varName],prefix=varName)\n",
    "    testVar_binary = pd.get_dummies(testSet[varName],prefix=varName)\n",
    "    \n",
    "    # On convertit nos tableaux binaires en Dataframe\n",
    "    trainVar_binary = pd.DataFrame(trainVar_binary)\n",
    "    testVar_binary = pd.DataFrame(testVar_binary)\n",
    "    \n",
    "    # On enleve la variable qualitative de nos jeux de donnees\n",
    "    trainSet_new = trainSet.drop([varName], axis=1)\n",
    "    testSet_new = testSet.drop([varName], axis=1)   \n",
    "\n",
    "    # On ajoute les nouvelles colonnes a nos jeux de donnees\n",
    "    trainSet_new = pd.concat([trainSet_new,trainVar_binary],axis=1)\n",
    "    testSet_new = pd.concat([testSet_new,testVar_binary],axis=1)\n",
    "    \n",
    "    # On identifie les colonnes manquantes dans le jeu de donnees test\n",
    "    missing_categories = set(trainSet_new) - set(testSet_new)\n",
    "\n",
    "    # On ajoute ces colonnes manquantes avec une valeur par defaut de 0\n",
    "    for c in missing_categories:\n",
    "        testSet_new[c] = 0\n",
    "\n",
    "    # On identifie les colonnes manquantes dans le jeu de donnees training\n",
    "    missing_categories = set(testSet_new) - set(trainSet_new)\n",
    "\n",
    "    # On ajoute ces colonnes manquantes avec une valeur par defaut de 0\n",
    "    for c in missing_categories:\n",
    "        trainSet_new[c] = 0\n",
    "\n",
    "\n",
    "    # On aligne nos jeux de donnees pour que les memes colonnes soient aux memes endroits\n",
    "    trainSet_new, testSet_new = trainSet_new.align(testSet_new, axis=1)\n",
    "    \n",
    "    # On retourne les jeux de donnees\n",
    "    return trainSet_new, testSet_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe les librairies\n",
    "import numpy as np # calcul matriciel\n",
    "import pandas as pd # structure des donnees\n",
    "import matplotlib.pyplot as plt # graphiques\n",
    "\n",
    "# Chemin vers les jeux de donnees en format csv\n",
    "pathTest = \"./test3.csv\"\n",
    "pathTrain = \"./train3.csv\"\n",
    "\n",
    "# On importe les jeux de donnees\n",
    "df_test = pd.read_csv(pathTest, sep=',', index_col=0)\n",
    "df_train = pd.read_csv(pathTrain, sep=',', index_col=0)\n",
    "\n",
    "# On enleve la colonne Name qui ne sert pas a l'analyse\n",
    "df_train = df_train.drop(['Name'],axis=1)\n",
    "\n",
    "\n",
    "# --- Variables Explicatives ---\n",
    "\n",
    "#    Name : Nom du jeu\n",
    "#    Platform : Console sur laquelle le jeu fonctionne\n",
    "#    Year_of_Release : Année de sortie du jeu\n",
    "#    Genre\n",
    "#    Publisher\n",
    "#    JP_Sales : Nombre de ventes du jeu au Japon en millions d’unités\n",
    "#    Other_Sales : Nombre de ventes du jeu ailleurs dans le monde : Afrique, Asie sans le Japon, Europe sans l’Union Européenne et Amérique du Sud en millions d’unités\n",
    "#    Critic_Score : Score donné par Metacritic\n",
    "#    Critic_Count : Nombre de critiques prises en compte pour estimer le Critic_score\n",
    "#    User_Score : Score donné par les usagers de Metacritic\n",
    "#    User_Count : Nombre d’usagers considérés pour estimer le User_Score\n",
    "#    Developer : Compagnie créatrice du jeu\n",
    "#    Rating : Classement ESRB (Entertainment Software Rating Board) ie à qui s’addresse le jeu (tout public, majeur, adolescents, etc) \n",
    "\n",
    "\n",
    "# --- Variables d'Interet ---\n",
    "\n",
    "#    NA_sales : Nombre de ventes du jeu en Amérique du Nord en millions d’unités\n",
    "#    Global_Sales : Nombre de ventes total du jeu en millions d’unités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories Platform (train) : 30\n",
      "Nbr. of categories Platform (test) : 27 \n",
      "\n",
      "Nbr. of categories Genre (train) : 12\n",
      "Nbr. of categories Genre (test) : 12 \n",
      "\n",
      "Nbr. of categories Publisher (train) : 552\n",
      "Nbr. of categories Publisher (test) : 280 \n",
      "\n",
      "Nbr. of categories Developer (train) : 1593\n",
      "Nbr. of categories Developer (test) : 677 \n",
      "\n",
      "Nbr. of categories Rating (train) : 8\n",
      "Nbr. of categories Rating (test) : 5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Ici on transforme nos variables qualitatives en tableau binaire\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Variable Qualitative #1 : Platform\n",
    "print(\"Nbr. of categories Platform (train) : %d\" %(df_train.Platform.nunique()))\n",
    "print(\"Nbr. of categories Platform (test) : %d \\n\" %(df_test.Platform.nunique()))\n",
    "df_train, df_test = qualiToBinary(df_train, df_test, 'Platform') # On binairise\n",
    "\n",
    "\n",
    "# Variable Qualitative #2 : Genre\n",
    "print(\"Nbr. of categories Genre (train) : %d\" %(df_train.Genre.nunique()))\n",
    "print(\"Nbr. of categories Genre (test) : %d \\n\" %(df_test.Genre.nunique()))\n",
    "df_train, df_test = qualiToBinary(df_train, df_test,'Genre') # On binairise\n",
    "\n",
    "\n",
    "# Variable Qualitative #3 : Publisher\n",
    "print(\"Nbr. of categories Publisher (train) : %d\" %(df_train.Publisher.nunique()))\n",
    "print(\"Nbr. of categories Publisher (test) : %d \\n\" %(df_test.Publisher.nunique()))\n",
    "df_train, df_test = qualiToBinary(df_train, df_test, 'Publisher') # On binairise\n",
    "\n",
    "\n",
    "# Variable Qualitative #4 : Developer\n",
    "print(\"Nbr. of categories Developer (train) : %d\" %(df_train.Developer.nunique()))\n",
    "print(\"Nbr. of categories Developer (test) : %d \\n\" %(df_test.Developer.nunique()))\n",
    "df_train, df_test = qualiToBinary(df_train, df_test,'Developer') # On binairise\n",
    "#df_train = df_train.drop(['Developer'], axis=1)\n",
    "#df_test = df_test.drop(['Developer'], axis=1)\n",
    "\n",
    "# Variable Qualitative #5 : Rating\n",
    "print(\"Nbr. of categories Rating (train) : %d\" %(df_train.Rating.nunique()))\n",
    "print(\"Nbr. of categories Rating (test) : %d \\n\" %(df_test.Rating.nunique()))\n",
    "df_train, df_test = qualiToBinary(df_train, df_test,'Rating') # On binairise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Ici on ajoute de nouvelles colonnes qui sont des combinaisons non-lineaires \n",
    "\n",
    "des autres colonnes ou de toute nouvelles informations.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# On ajoute une colonne qui comptabilise le nombre de NaN pour chaque jeu video\n",
    "nbrOfNaN_train = ~(df_train.T.iloc[:].isna().sum().astype(bool))\n",
    "df_train['Contains_NaN'] = nbrOfNaN_train.astype(np.uint8)\n",
    "\n",
    "nbrOfNaN_test = ~(df_test.T.iloc[:].isna().sum().astype(bool))\n",
    "df_test['Contains_NaN'] = nbrOfNaN_test.astype(np.uint8)\n",
    "\n",
    "df_train['JP_Sales2'] = df_train.JP_Sales**2\n",
    "df_test['JP_Sales2'] = df_test.JP_Sales**2\n",
    "\n",
    "df_train['Other_Sales2'] = df_train.Other_Sales**2\n",
    "df_test['Other_Sales2'] = df_test.Other_Sales**2\n",
    "\n",
    "df_train['JP_Sales_Other_Sales'] = df_train.Other_Sales*df_train.JP_Sales\n",
    "df_test['JP_Sales_Other_Sales'] = df_test.Other_Sales*df_test.JP_Sales\n",
    "\n",
    "df_train['Other_Sales_Year_of_Release'] = df_train.Other_Sales*df_train.Year_of_Release\n",
    "df_test['Other_Sales_Year_of_Release'] = df_test.Other_Sales*df_test.Year_of_Release\n",
    "\n",
    "df_train['JP_Sales_Year_of_Release'] = df_train.JP_Sales*df_train.Year_of_Release\n",
    "df_test['JP_Sales_Year_of_Release'] = df_test.JP_Sales*df_test.Year_of_Release\n",
    "\n",
    "df_train['JP_Sales_Year2_of_Release2'] = df_train.JP_Sales**2*df_train.Year_of_Release**2\n",
    "df_test['JP_Sales_Year2_of_Release2'] = df_test.JP_Sales**2*df_test.Year_of_Release**2\n",
    "\n",
    "df_train['Critic_Score9_Critic_Count'] = df_train.Critic_Score**9*df_train.Critic_Count\n",
    "df_test['Critic_Score9_Critic_Count'] = df_test.Critic_Score**9*df_test.Critic_Count\n",
    "\n",
    "df_train['Critic_Score12_Critic_Count'] = df_train.Critic_Score**12*df_train.Critic_Count\n",
    "df_test['Critic_Score12_Critic_Count'] = df_test.Critic_Score**12*df_test.Critic_Count\n",
    "\n",
    "df_train['Critic_Score_Critic_Count'] = df_train.Critic_Score*df_train.Critic_Count\n",
    "df_test['Critic_Score_Critic_Count'] = df_test.Critic_Score*df_test.Critic_Count\n",
    "\n",
    "df_train['User_Score_User_Count'] = df_train.User_Score*df_train.User_Count\n",
    "df_test['User_Score_User_Count'] = df_test.User_Score*df_test.User_Count\n",
    "\n",
    "df_train['User_Score2'] = df_train.User_Score**2\n",
    "df_test['User_Score2'] = df_test.User_Score**2\n",
    "\n",
    "df_train['User_Count2'] = df_train.User_Count**2\n",
    "df_test['User_Count2'] = df_test.User_Count**2\n",
    "\n",
    "df_train['Critic_Count2'] = df_train.Critic_Count**2\n",
    "df_test['Critic_Count2'] = df_test.Critic_Count**2\n",
    "\n",
    "df_train['Critic_Score2'] = df_train.Critic_Score**2\n",
    "df_test['Critic_Score2'] = df_test.Critic_Score**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Ici on remplace les NaN par la mediane des valeurs\n",
    "\n",
    "\"\"\"\n",
    "df_train = df_train.fillna(df_train.median())\n",
    "df_test = df_test.fillna(df_train.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On fait une matrice de nos variables\n",
    "X_train = df_train.drop(['NA_Sales','Global_Sales'],axis=1)\n",
    "X_test = df_test.drop(['NA_Sales','Global_Sales'],axis=1)\n",
    "\n",
    "# On isole nos variables d'interet\n",
    "Y1 = df_train.Global_Sales.values\n",
    "Y2 = df_train.NA_Sales.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# On transforme nos variables pour moyenne = 0 et var = 1\n",
    "X_train_std = scaler(X_train.values)\n",
    "X_test_std = scaler(X_test.values)\n",
    "\n",
    "# On enleve les colonnes ou nous avons des NaN (train)\n",
    "columnNaN = ~np.all(np.isnan(X_train_std), axis=0)\n",
    "X_train_std = X_train_std[:,columnNaN]\n",
    "X_test_std = X_test_std[:,columnNaN]\n",
    "\n",
    "# On enleve les colonnes ou nous avons des NaN (test)\n",
    "columnNaN = ~np.all(np.isnan(X_test_std), axis=0)\n",
    "X_train_std = X_train_std[:,columnNaN]\n",
    "X_test_std = X_test_std[:,columnNaN]\n",
    "\"\"\"\n",
    "\n",
    "# On transforme nos variables pour moyenne = 0 et var = 1\n",
    "X_train_std = X_train.values\n",
    "X_test_std = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d2d278c6d529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# On calcule la regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_est\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train_std\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# On calcule notre erreur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-c4c257e1bd7c>\u001b[0m in \u001b[0;36mridge_regression\u001b[0;34m(y, x, l, minY, maxY)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# On calcule les coefficients de regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_withOne\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# On calcule les predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparametre 1 (Global_Sales) - plancher des predictions\n",
    "best_RMSE = 1\n",
    "best_minY1 = 0\n",
    "\n",
    "maxY = np.max(Y1)\n",
    "l = 0.5\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    # On met a jour l'hyperparametre\n",
    "    minY = 0.05 + 0.001*i\n",
    "    \n",
    "    # On calcule la regression\n",
    "    B, Y_est = ridge_regression(Y1,X_train_std,l,minY,maxY)\n",
    "    \n",
    "    # On calcule notre erreur\n",
    "    rmse = RMSE(Y1,Y_est)\n",
    "    \n",
    "    if(rmse < best_RMSE):\n",
    "        best_RMSE = rmse\n",
    "        best_minY1 = minY\n",
    "\n",
    "        \n",
    "print(\"Global Sales, minY = %.3f\" % best_minY1)\n",
    "print(\"Global Sales, RMSE = %.3f \\n\" % best_RMSE)\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparametre 1 (NA_Sales) - plancher des predictions\n",
    "best_RMSE = 1\n",
    "best_minY2 = 0\n",
    "\n",
    "maxY = np.max(Y2)\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    # On met a jour l'hyperparametre\n",
    "    minY = 0.01 + 0.001*i\n",
    "    \n",
    "    # On calcule la regression\n",
    "    B, Y_est = ridge_regression(Y2,X_train_std,l,minY,maxY)\n",
    "    \n",
    "    # On calcule notre erreur\n",
    "    rmse = RMSE(Y2,Y_est)\n",
    "    \n",
    "    if(rmse < best_RMSE):\n",
    "        best_RMSE = rmse\n",
    "        best_minY2 = minY\n",
    "\n",
    "        \n",
    "print(\"NA Sales, minY = %.3f\" % best_minY2)\n",
    "print(\"NA Sales, RMSE = %.3f\" % best_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Sales, maxY = 82.5407\n",
      "NA Sales, maxY = 41.3598\n"
     ]
    }
   ],
   "source": [
    "# Hyperparametre 2 (Global_Sales) - plafond des predictions\n",
    "best_maxY1 = np.max(Y1)\n",
    "print(\"Global Sales, maxY = %.4f\" % best_maxY1)\n",
    "\n",
    "# Hyperparametre 2 (NA_Sales) - plafond des predictions\n",
    "best_maxY2 = np.max(Y2)\n",
    "print(\"NA Sales, maxY = %.4f\" % best_maxY2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparametre 3 (Global_Sales) - parametre de penalisation\n",
    "nbr_ite = 20\n",
    "\n",
    "best_RMSE = 1\n",
    "best_l1 = 0\n",
    "rmse1 = np.zeros(nbr_ite)\n",
    "r2_prev1 = np.zeros(nbr_ite)\n",
    "\n",
    "maxY = np.max(Y1)\n",
    "\n",
    "for i in range(nbr_ite):\n",
    "    \n",
    "    # On met a jour l'hyperparametre\n",
    "    minl = 0.05*i\n",
    "    \n",
    "    # On calcule la regression\n",
    "    B, Y_est = ridge_regression(Y1,X_train_std,minl,0,best_maxY1)\n",
    "    \n",
    "    # On calcule notre erreur\n",
    "    rmse = RMSE(Y1,Y_est)\n",
    "    rmse1[i] = rmse\n",
    "    r2_prev1[i] = R2_prev(Y1,Y_est,X_train_std)\n",
    "    \n",
    "    if(rmse < best_RMSE):\n",
    "        best_RMSE = rmse\n",
    "        best_l1 = minl\n",
    "\n",
    "        \n",
    "print(\"Global Sales, minl = %.4f\" % best_l1)\n",
    "print(\"Global Sales, RMSE = %.4f \\n\" % best_RMSE)\n",
    "\n",
    "\n",
    "# Hyperparametre 1 (NA_Sales) - parametre de penalisation\n",
    "best_RMSE = 1\n",
    "best_l2 = 0\n",
    "rmse2 = np.zeros(nbr_ite)\n",
    "r2_prev2 = np.zeros(nbr_ite)\n",
    "\n",
    "maxY = np.max(Y2)\n",
    "\n",
    "for i in range(nbr_ite):\n",
    "    \n",
    "    # On met a jour l'hyperparametre\n",
    "    minl = 0.05*i\n",
    "    \n",
    "    # On calcule la regression\n",
    "    B, Y_est = ridge_regression(Y2,X_train_std,minl,0,best_maxY2)\n",
    "    \n",
    "    # On calcule notre erreur\n",
    "    rmse = RMSE(Y2,Y_est)\n",
    "    rmse2[i] = rmse\n",
    "    r2_prev2[i] = R2_prev(Y2,Y_est,X_train_std)\n",
    "    \n",
    "    if(rmse < best_RMSE):\n",
    "        best_RMSE = rmse\n",
    "        best_l2 = minl\n",
    "\n",
    "        \n",
    "print(\"NA Sales, minl = %.4f\" % best_l2)\n",
    "print(\"NA Sales, RMSE = %.4f \\n\" % best_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.array(list(range(nbr_ite)))\n",
    "index = index*0.05\n",
    "\n",
    "plt.plot(index, rmse1, 'or')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('RMSE')\n",
    "plt.ylim(0,1)\n",
    "plt.title('Global Sales')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(index, r2_prev1, 'or')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('R2 Prev')\n",
    "plt.ylim(0,1)\n",
    "plt.title('Global Sales')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(index, rmse2, 'or')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('RMSE')\n",
    "plt.ylim(0,1)\n",
    "plt.title('NA Sales')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(index, r2_prev2, 'or')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('R2 Prev')\n",
    "plt.ylim(0,1)\n",
    "plt.title('NA Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Les graphiques ci-haut nous permettent de choisir un parametre de penalisation qui minimise le RMSE sans overfit, \n",
    "on choisit 0.5\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  0.44048272558618395\n"
     ]
    }
   ],
   "source": [
    "# Global Sales\n",
    "B1, Y_est1 = ridge_regression(Y1,X_train_std,1000,0,best_maxY1)\n",
    "\n",
    "# Clip\n",
    "global_data = np.dot(X_train_std,np.delete(B1,0)) + B1[0]\n",
    "global_data = global_data.clip(best_minY1,best_maxY1)\n",
    "\n",
    "print(\"RMSE : \",RMSE(Y1,global_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  0.27527940168018245\n"
     ]
    }
   ],
   "source": [
    "# NA Sales\n",
    "B2, Y_est2 = ridge_regression(Y2,X_train_std,1000,best_minY2,best_maxY2)\n",
    "\n",
    "# Clip\n",
    "na_data = np.dot(X_train_std,np.delete(B2,0)) + B2[0]\n",
    "na_data = na_data.clip(best_minY2,best_maxY2)\n",
    "\n",
    "print(\"RMSE : \",RMSE(Y2,na_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Processing du jeu de donnees test\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# On genere nos predictions\n",
    "global_data_test = np.dot(X_test_std,np.delete(B1,0)) + B1[0]\n",
    "na_data_test = np.dot(X_test_std,np.delete(B2,0)) + B2[0]\n",
    "\n",
    "# On borne nos predictions\n",
    "global_data_test = global_data_test.clip(best_minY1,best_maxY1)  \n",
    "\n",
    "# On borne nos predictions\n",
    "na_data_test = na_data_test.clip(best_minY2,best_maxY2)\n",
    "\n",
    "# On vient prendre le dataframe comme canva\n",
    "df_test_estimated = pd.DataFrame([df_test.NA_Sales,df_test.Global_Sales]).copy().T\n",
    "\n",
    "# On assigne nos predictions aux colonnes\n",
    "df_test_estimated['Global_Sales'] = global_data_test\n",
    "df_test_estimated['NA_Sales'] = na_data_test\n",
    "\n",
    "# On sauvegarde nos predictions\n",
    "df_test_estimated.to_csv(\"test_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
