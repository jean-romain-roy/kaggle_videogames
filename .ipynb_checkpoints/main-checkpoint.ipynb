{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe les librairies\n",
    "import numpy as np # calcul matriciel\n",
    "import pandas as pd # structure des donnees\n",
    "\n",
    "# Chemin vers les jeux de donnees en format csv\n",
    "pathTest = \"./test3.csv\"\n",
    "pathTrain = \"./train3.csv\"\n",
    "\n",
    "# On importe les jeux de donnees\n",
    "df_test = pd.read_csv(pathTest, sep=',', index_col=0)\n",
    "df_train = pd.read_csv(pathTrain, sep=',', index_col=0)\n",
    "\n",
    "# On enleve la colonne Name qui ne sert pas a l'analyse\n",
    "df_train = df_train.drop(['Name'],axis=1)\n",
    "\n",
    "\n",
    "# --- Variables Explicatives ---\n",
    "\n",
    "#    Name : Nom du jeu\n",
    "#    Platform : Console sur laquelle le jeu fonctionne\n",
    "#    Year_of_Release : Année de sortie du jeu\n",
    "#    Genre\n",
    "#    Publisher\n",
    "#    JP_Sales : Nombre de ventes du jeu au Japon en millions d’unités\n",
    "#    Other_Sales : Nombre de ventes du jeu ailleurs dans le monde : Afrique, Asie sans le Japon, Europe sans l’Union Européenne et Amérique du Sud en millions d’unités\n",
    "#    Critic_Score : Score donné par Metacritic\n",
    "#    Critic_Count : Nombre de critiques prises en compte pour estimer le Critic_score\n",
    "#    User_Score : Score donné par les usagers de Metacritic\n",
    "#    User_Count : Nombre d’usagers considérés pour estimer le User_Score\n",
    "#    Developer : Compagnie créatrice du jeu\n",
    "#    Rating : Classement ESRB (Entertainment Software Rating Board) ie à qui s’addresse le jeu (tout public, majeur, adolescents, etc) \n",
    "\n",
    "\n",
    "# --- Variables d'Interet ---\n",
    "\n",
    "#    NA_sales : Nombre de ventes du jeu en Amérique du Nord en millions d’unités\n",
    "#    Global_Sales : Nombre de ventes total du jeu en millions d’unités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Regression de Ridge (L2) par methode des moindres carres permettant la penalisation du surapprentissage.\n",
    "\n",
    "Arguments :\n",
    "\n",
    "    y : Variable d'interet (dimension : n x 1)\n",
    "    x : matrice contenant nos variables explicatives (dimension : n x (p+1))\n",
    "    l : facteur qui permet de diminuer l'amplitude des coefficients de regression\n",
    "    \n",
    "    ou,\n",
    "        n : nombre d'observation\n",
    "        p : nombre de variable explicative       \n",
    "        \n",
    "\n",
    "Retourne : \n",
    "\n",
    "    B = Matrices de nos coefficiants de regression (dimension (p+1) x 1)\n",
    "\n",
    "    Pour estimer la valeur de y_new avec de nouvelles observations (i.e. x_new) on a,\n",
    "        y_new = np.dot(x_new,B)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ridge_regression(y,x,l):\n",
    "    \n",
    "    # Nombre de variable explicative + colonne de 1\n",
    "    nbr = len(x.T)\n",
    "    \n",
    "    # Matrice identite de taille p+1\n",
    "    identi = l*np.identity(nbr) \n",
    "    identi[0][0] = 0 # on met un zero vis a vis le coeff b0 qui est l'ordonnee a l'origine\n",
    "    \n",
    "    # On calcule la matrice de variance-covariance avec un biais de penalite\n",
    "    C = np.linalg.inv(np.dot(x.T,x) + np.dot(l,identi))\n",
    "    \n",
    "    # On calcule les coefficients de regression\n",
    "    B = np.dot(np.dot(C,x.T),y)\n",
    "    \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Fonction permettant de transformer une matrice pour que ses colonnes aient une moyenne = 0 et une variance = 1\n",
    "\n",
    "Argument:\n",
    "    dataset : le jeu de donnees a transformer\n",
    "    \n",
    "Retourne :\n",
    "    dataset_std : le jeu de donnees transforme\n",
    "\n",
    "\"\"\"\n",
    "def scaler(dataset):\n",
    "    \n",
    "    # On calcule la moyenne pour chaque colonne\n",
    "    u = np.array([np.mean(dataset, axis=0)])\n",
    "    \n",
    "    # On soustrait les colonnes du jeu de donnees par ses moyennes\n",
    "    dataset_std = dataset - u\n",
    "    \n",
    "    # On calcule la variance pour chaque colonne\n",
    "    sigma = np.array([np.std(dataset_std, axis=0)])\n",
    "    \n",
    "    # On divise les colonnes du jeu de donnees par ses variances\n",
    "    dataset_std = dataset_std/sigma\n",
    "    \n",
    "    # Si contient des NaN cause par une division par 0 (si la variance etait de 0)\n",
    "    if(np.isnan(dataset_std).any()):\n",
    "        print(\"Error will contain NaN\") # On affiche le message d'erreur\n",
    "        reducedCols = ~np.all(np.isnan(dataset_std),axis=0)\n",
    "        return reducedCols # On retourne les index ou il y a presence de NaN\n",
    "    \n",
    "    return dataset_std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--- Variable Qualitative -> Table Binaire --- \n",
    "\n",
    "Afin de traiter nos variables qualitatives, nous avons une fonction qui la tranforme en table binaire \n",
    "avec un one bit encoder. \n",
    "\n",
    "    Disons que la variable explicative peut prendre 4 valeurs {Bleu, Vert, Jaune, Rouge},\n",
    "        \n",
    "          ID        X1\n",
    "        ----------------\n",
    "          1        Bleu\n",
    "          2        Bleu\n",
    "          3        Vert\n",
    "          4        Jaune\n",
    "          5        Vert\n",
    "          6        Rouge          \n",
    "         ...\n",
    "    \n",
    "    Suite a un encodage en 1 bit notre variable explicative X1 devient,\n",
    "    \n",
    "          ID        Bleu        Vert        Jaune        Rouge\n",
    "        -------------------------------------------------------\n",
    "          1          1           0            0            0\n",
    "          2          1           0            0            0\n",
    "          3          0           1            0            0\n",
    "          4          0           0            1            0\n",
    "          5          0           1            0            0\n",
    "          6          0           0            0            1\n",
    "                                ...\n",
    "                                \n",
    "\n",
    "Nous nous basons sur la fonction get_dummies() de la librairie Panda pour faire ceci.\n",
    "\n",
    "\n",
    "Arguments,\n",
    "\n",
    "    trainSet : le jeu de donnees de training\n",
    "    testSet : le jeu de donnees de test\n",
    "    varName : la variable qualitative que nous souhaitons transformer en tableau binaire\n",
    "    \n",
    "    \n",
    "Retourne, \n",
    "    trainSet_new : le jeu de donnees de training ou la variable qualitative a ete remplacee par un tableau binaire\n",
    "    testSet_new : le jeu de donnees de test ou la variable qualitative a ete remplacee par un tableau binaire\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def qualiToBinary(trainSet, testSet, varName):\n",
    "    \n",
    "    # Genere la table binaire de la variable qualitative\n",
    "    trainVar_binary = pd.get_dummies(trainSet[varName],prefix=varName)\n",
    "    testVar_binary = pd.get_dummies(testSet[varName],prefix=varName)\n",
    "    \n",
    "    # On convertit nos tableaux binaires en Dataframe\n",
    "    trainVar_binary = pd.DataFrame(trainVar_binary)\n",
    "    testVar_binary = pd.DataFrame(testVar_binary)\n",
    "    \n",
    "    # On enleve la variable qualitative de nos jeux de donnees\n",
    "    trainSet_new = trainSet.drop([varName], axis=1)\n",
    "    testSet_new = testSet.drop([varName], axis=1)   \n",
    "\n",
    "    # On ajoute les nouvelles colonnes a nos jeux de donnees\n",
    "    trainSet_new = pd.concat([trainSet_new,trainVar_binary],axis=1)\n",
    "    testSet_new = pd.concat([testSet_new,testVar_binary],axis=1)\n",
    "    \n",
    "    # On identifie les colonnes manquantes dans le jeu de donnees test\n",
    "    missing_categories = set(trainSet_new) - set(testSet_new)\n",
    "\n",
    "    # On ajoute ces colonnes manquantes avec une valeur par defaut de 0\n",
    "    for c in missing_categories:\n",
    "        testSet_new[c] = 0\n",
    "\n",
    "    # On identifie les colonnes manquantes dans le jeu de donnees training\n",
    "    missing_categories = set(testSet_new) - set(trainSet_new)\n",
    "\n",
    "    # On ajoute ces colonnes manquantes avec une valeur par defaut de 0\n",
    "    for c in missing_categories:\n",
    "        trainSet_new[c] = 0\n",
    "\n",
    "\n",
    "    # On aligne nos jeux de donnees pour que les memes colonnes soient aux memes endroits\n",
    "    trainSet_new, testSet_new = trainSet_new.align(testSet_new, axis=1)\n",
    "    \n",
    "    # On retourne les jeux de donnees\n",
    "    return trainSet_new, testSet_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "L'algorithme R2_prev implemente une validation croise sur l'ensemble du jeu de donnees fournis\n",
    "\n",
    "Arguments:\n",
    "    Y : La variable d'interet\n",
    "    W : Notre estimation de la variable d'interet\n",
    "    X : Nos variables explicatives\n",
    "    \n",
    "\n",
    "\n",
    "Retourne:\n",
    "\n",
    "    R2_prev : Une mesure bornee entre [-inf, 1] qui represente le pouvoir predictif de nos estimations\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def R2_prev(Y,W,X):\n",
    "    \n",
    "    # The variance-covariance\n",
    "    C = np.linalg.inv(np.dot(X.T,X))\n",
    "    \n",
    "    # Hat Matrix (X*B = H*Y)\n",
    "    H = np.dot(np.dot(X,C),X.T)\n",
    "    \n",
    "    # We are only interested in the (i,i) values\n",
    "    H = np.diagonal(H)\n",
    "    \n",
    "    # Mean of Y\n",
    "    y_S = np.sum(Y)/float(len(Y))\n",
    "    \n",
    "    num = 0.0\n",
    "    for i in range(len(Y)):\n",
    "        num += ((Y[i] - W[i])/float(1 - H[i]))**2\n",
    "\n",
    "    denom = 0.0\n",
    "    for i in range(len(Y)):\n",
    "        denom += (Y[i] - y_S)**2\n",
    "    \n",
    "    R2_prev = (1 - num/denom)\n",
    "    \n",
    "    return R2_prev\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Selon le document MTH3302_CriteresProjet.pdf nous sommes evalues par notre root mean squared error (RMSE).\n",
    "On trouve ici son implementation.\n",
    "\n",
    "Arguments :\n",
    "    Y : La variable d'interet\n",
    "    W : Notre estimation de la variable d'interet\n",
    "    \n",
    "    \n",
    "Retourne:\n",
    "    rmse : L'erreur moyenne de nos predictions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def RMSE(Y,W):\n",
    "\n",
    "    n = len(Y)\n",
    "\n",
    "    total = 0.0\n",
    "    for i in range(n):\n",
    "        total += (Y[i] - W[i])**2\n",
    "        \n",
    "    rmse = total/float(n)\n",
    "        \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories (train) : 30\n",
      "Nbr. of categories (test) : 27\n"
     ]
    }
   ],
   "source": [
    "# Variable Qualitative #1 : Platform\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Platform.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Platform.nunique()))\n",
    "\n",
    "df_train, df_test = qualiToBinary(df_train, df_test, 'Platform') # On binairise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories (train) : 12\n",
      "Nbr. of categories (test) : 12\n"
     ]
    }
   ],
   "source": [
    "# Variable Qualitative #2 : Genre\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Genre.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Genre.nunique()))\n",
    "\n",
    "df_train, df_test = qualiToBinary(df_train, df_test,'Genre') # On binairise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories (train) : 552\n",
      "Nbr. of categories (test) : 280\n"
     ]
    }
   ],
   "source": [
    "# Variable Qualitative #3 : Publisher\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Publisher.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Publisher.nunique()))\n",
    "\n",
    "df_train, df_test = qualiToBinary(df_train, df_test, 'Publisher') # On binairise\n",
    "#df_test = df_test.drop(['Publisher'],axis=1)\n",
    "#df_train = df_train.drop(['Publisher'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories (train) : 1593\n",
      "Nbr. of categories (test) : 677\n"
     ]
    }
   ],
   "source": [
    "# Variable Qualitative #4 : Developer\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Developer.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Developer.nunique()))\n",
    "\n",
    "df_train, df_test = qualiToBinary(df_train, df_test,'Developer') # On binairise\n",
    "#df_test = df_test.drop(['Developer'],axis=1)\n",
    "#df_train = df_train.drop(['Developer'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories (train) : 8\n",
      "Nbr. of categories (test) : 5\n"
     ]
    }
   ],
   "source": [
    "# Variable Qualitative #5 : Rating\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Rating.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Rating.nunique()))\n",
    "\n",
    "df_train, df_test = qualiToBinary(df_train, df_test,'Rating') # On binairise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ajoute une colonne qui comptabilise le nombre de NaN pour chaque jeu video\n",
    "nbrOfNaN_train = ~(df_train.T.iloc[:].isna().sum().astype(bool))\n",
    "df_train['Complete_Data'] = nbrOfNaN_train.astype(np.uint8)\n",
    "\n",
    "nbrOfNaN_test = ~(df_test.T.iloc[:].isna().sum().astype(bool))\n",
    "df_test['Complete_Data'] = nbrOfNaN_test.astype(np.uint8)\n",
    "\n",
    "df_train['JP_Sales2'] = df_train.JP_Sales**2\n",
    "df_test['JP_Sales2'] = df_test.JP_Sales**2\n",
    "\n",
    "df_train['Other_Sales2'] = df_train.Other_Sales**2\n",
    "df_test['Other_Sales2'] = df_test.Other_Sales**2\n",
    "\n",
    "df_train['JP_Sales_Other_Sales'] = df_train.Other_Sales*df_train.JP_Sales\n",
    "df_test['JP_Sales_Other_Sales'] = df_test.Other_Sales*df_test.JP_Sales\n",
    "\n",
    "df_train['Other_Sales_Year_of_Release'] = df_train.Other_Sales*df_train.Year_of_Release\n",
    "df_test['Other_Sales_Year_of_Release'] = df_test.Other_Sales*df_test.Year_of_Release\n",
    "\n",
    "df_train['JP_Sales_Year_of_Release'] = df_train.JP_Sales*df_train.Year_of_Release\n",
    "df_test['JP_Sales_Year_of_Release'] = df_test.JP_Sales*df_test.Year_of_Release\n",
    "\n",
    "df_train['Critic_Score9_Critic_Count'] = df_train.Critic_Score**9*df_train.Critic_Count\n",
    "df_test['Critic_Score9_Critic_Count'] = df_test.Critic_Score**9*df_test.Critic_Count\n",
    "\n",
    "df_train['Critic_Score_Critic_Count'] = df_train.Critic_Score*df_train.Critic_Count\n",
    "df_test['Critic_Score_Critic_Count'] = df_test.Critic_Score*df_test.Critic_Count\n",
    "\n",
    "df_train['User_Score_User_Count'] = df_train.User_Score*df_train.User_Count\n",
    "df_test['User_Score_User_Count'] = df_test.User_Score*df_test.User_Count\n",
    "\n",
    "df_train['User_Score2'] = df_train.User_Score**2\n",
    "df_test['User_Score2'] = df_test.User_Score**2\n",
    "\n",
    "df_train['User_Count2'] = df_train.User_Count**2\n",
    "df_test['User_Count2'] = df_test.User_Count**2\n",
    "\n",
    "df_train['Critic_Count2'] = df_train.Critic_Count**2\n",
    "df_test['Critic_Count2'] = df_test.Critic_Count**2\n",
    "\n",
    "df_train['Critic_Score2'] = df_train.Critic_Score**2\n",
    "df_test['Critic_Score2'] = df_test.Critic_Score**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici on remplace les NaN par la mediane de leur colonne\n",
    "df_train_clean = df_train.fillna(df_train.median()).copy()\n",
    "df_test_clean = df_test.fillna(df_test.median()).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jean-romain/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error will contain NaN\n",
      "Error will contain NaN\n"
     ]
    }
   ],
   "source": [
    "# On fait une matrice de nos variables\n",
    "X_train = df_train_clean.drop(['NA_Sales','Global_Sales'],axis=1)\n",
    "X_test = df_test_clean.drop(['NA_Sales','Global_Sales'],axis=1)\n",
    "\n",
    "# On les standardise\n",
    "indexes = scaler(X_test.values)\n",
    "X_train = X_train.T[np.array(indexes)].T\n",
    "X_test = X_test.T[np.array(indexes)].T\n",
    "\n",
    "indexes = scaler(X_train.values)\n",
    "X_train = X_train.T[np.array(indexes)].T\n",
    "X_test = X_test.T[np.array(indexes)].T\n",
    "\n",
    "X_test = scaler(X_test.values)\n",
    "X_train = scaler(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ajoute une colonne de 1\n",
    "Ones = np.array([np.ones(len(X_train))]).T\n",
    "X_train_one = np.concatenate((Ones,X_train),axis = 1)\n",
    "\n",
    "Ones = np.array([np.ones(len(X_test))]).T\n",
    "X_test_one = np.concatenate((Ones,X_test),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX B :  1.2728235329862545\n",
      "R2_prev (Global) : 0.7800\n",
      "RMSE (Global) : 0.3590\n"
     ]
    }
   ],
   "source": [
    "Y1 = df_train_clean.Global_Sales.values\n",
    "\n",
    "for i in range(500):\n",
    "    B1 = ridge_regression(Y1,X_train_one,0.1+0.01*i)\n",
    "\n",
    "    # Clip to min 0\n",
    "    global_data = np.dot(X_train_one,B1)\n",
    "    global_data = global_data.clip(min=0)\n",
    "\n",
    "    print(\"i = \",0.1+0.01*i)\n",
    "    print(\"R2_prev (Global) : %.4f\" % (R2_prev(Y1,global_data,X_train)))\n",
    "    print(\"RMSE (Global) : %.4f\" % (RMSE(Y1,global_data)))\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2 = df_train_clean.NA_Sales.values\n",
    "\n",
    "for i in range(500):\n",
    "    B2 = ridge_regression(Y2,X_train_one,0.1+0.01*i)\n",
    "\n",
    "    # Clip to min 0\n",
    "    na_data = np.dot(X_train_one,B2)\n",
    "    na_data = na_data.clip(min=0)\n",
    "\n",
    "    print(\"i = \",0.1+0.01*i)\n",
    "    print(\"R2_prev (NA) : %.4f\" % (R2_prev(Y2,na_data,X_train)))\n",
    "    print(\"RMSE (NA) : %.4f\" % (RMSE(Y2,na_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Processing du jeu de donnees test\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# On genere nos predictions\n",
    "global_data_test = np.dot(X_test_one,B1)\n",
    "na_data_test = np.dot(X_test_one,B2)\n",
    "\n",
    "# On enleve les valeurs negatives\n",
    "global_data_test = global_data_test.clip(min=0)  \n",
    "\n",
    "# On enleve les valeurs negatives\n",
    "na_data_test = na_data_test.clip(min=0)\n",
    "\n",
    "# On vient prendre le dataframe comme canva\n",
    "df_test_estimated = pd.DataFrame([df_test.NA_Sales,df_test.Global_Sales]).copy().T\n",
    "\n",
    "# On assigne nos predictions aux colonnes\n",
    "df_test_estimated['Global_Sales'] = global_data_test\n",
    "df_test_estimated['NA_Sales'] = na_data_test\n",
    "\n",
    "# On sauvegarde nos predictions\n",
    "df_test_estimated.to_csv(\"test_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
