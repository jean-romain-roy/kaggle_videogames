{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe les librairies utiles\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# Chemin vers les csv\n",
    "pathTest = \"./test3.csv\"\n",
    "pathTrain = \"./train3.csv\"\n",
    "\n",
    "# Importe les jeux de donnees\n",
    "df_test = pd.read_csv(pathTest, sep=',', index_col=0)\n",
    "df_train = pd.read_csv(pathTrain, sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n--- Variables Explicatives ---\\nName : Nom du jeu\\nPlatform : Console sur laquelle le jeu fonctionne\\nYear of release : Année de sortie du jeu\\nGenre\\nPublisher\\nJP_sales : Nombre de ventes du jeu au Japon en millions d’unités\\nOther sales : Nombre de ventes du jeu ailleurs dans le monde : Afrique, Asie sans le Japon, Europe sans l’Union Européenne et Amérique du Sud en millions d’unités\\nCritic_score : Score donné par Metacritic\\nCritic_count : Nombre de critiques prises en compte pour estimer le Critic_score\\nUser_Score : Score donné par les usagers de Metacritic\\nUser_Count : Nombre d’usagers considérés pour estimer le User_Score\\nDeveloper : Compagnie créatrice du jeu\\nRating : Classement ESRB (Entertainment Software Rating Board) ie à qui s’addresse le jeu (tout public, majeur, adolescents, etc) \\n\\n--- Variable d'interet ---\\nNA_sales : Nombre de ventes du jeu en Amérique du Nord en millions d’unités\\nGlobal_Sales : Nombre de ventes total du jeu en millions d’unités\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "--- Variables Explicatives ---\n",
    "Name : Nom du jeu\n",
    "Platform : Console sur laquelle le jeu fonctionne\n",
    "Year of release : Année de sortie du jeu\n",
    "Genre\n",
    "Publisher\n",
    "JP_sales : Nombre de ventes du jeu au Japon en millions d’unités\n",
    "Other sales : Nombre de ventes du jeu ailleurs dans le monde : Afrique, Asie sans le Japon, Europe sans l’Union Européenne et Amérique du Sud en millions d’unités\n",
    "Critic_score : Score donné par Metacritic\n",
    "Critic_count : Nombre de critiques prises en compte pour estimer le Critic_score\n",
    "User_Score : Score donné par les usagers de Metacritic\n",
    "User_Count : Nombre d’usagers considérés pour estimer le User_Score\n",
    "Developer : Compagnie créatrice du jeu\n",
    "Rating : Classement ESRB (Entertainment Software Rating Board) ie à qui s’addresse le jeu (tout public, majeur, adolescents, etc) \n",
    "\n",
    "--- Variable d'interet ---\n",
    "NA_sales : Nombre de ventes du jeu en Amérique du Nord en millions d’unités\n",
    "Global_Sales : Nombre de ventes total du jeu en millions d’unités\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--- Evaluation des predictions --- \n",
    "\n",
    "Dans le document MTH3302_CriteresProjet-1.pdf on nous informe que la precision\n",
    "de nos estimations sera evaluee avec le root mean square error (RMSE)\n",
    "\n",
    "On definit cette fonction ci-bas,\n",
    "\n",
    "    Y : Variable d'interet\n",
    "    W : Predictions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def RMSE(Y,W):\n",
    "    \n",
    "    # Nombre d'observations\n",
    "    n = len(Y)\n",
    "    \n",
    "    total = 0.0\n",
    "    for i in range(n):\n",
    "        total += (Y[i] - W[i])**2\n",
    "    \n",
    "    mean = total/float(n)\n",
    "    \n",
    "    print(\"Mean Value = %.2f, RMSE = %.2f\" % (np.mean(Y),mean))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--- Variable Qualitative -> Table Binaire --- \n",
    "\n",
    "Afin de traiter nos variables qualitatives, nous les transformons en table binaire.\n",
    "Nous nous basons sur la fonction get_dummies de la librairies Panda pour arriver a nos fins.\n",
    "\n",
    "Input,\n",
    "    trainSet : le dataframe complet contenant le jeu de donnees de training avec toutes ses colonnes\n",
    "    testSet : le dataframe complet contenant le jeu de donnees de training avec toutes ses colonnes\n",
    "    trainVar : la variable qualitative que nous souhaitons transformer en tableau de variable binaire\n",
    "    testVar : la variable qualitative que nous souhaitons transformer en tableau de variable binaire\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def binerizedQualitativeVariable(trainSet, testSet, trainVar, testVar):\n",
    "    \n",
    "    # Generate Binary Table\n",
    "    trainVar_binary = pd.get_dummies(trainVar)\n",
    "    testVar_binary = pd.get_dummies(testVar)\n",
    "\n",
    "    # We concatenate the new columns\n",
    "    trainSet = pd.concat([trainSet,pd.DataFrame(trainVar_binary)],axis=1)\n",
    "    testSet = pd.concat([testSet,pd.DataFrame(testVar_binary)],axis=1)\n",
    "\n",
    "    # Get missing columns in the test set\n",
    "    missing_categories = set(trainSet) - set(testSet)\n",
    "\n",
    "    # Add the missing columns in test set with default value equal to 0\n",
    "    for c in missing_categories:\n",
    "        testSet[c] = 0\n",
    "\n",
    "    # Get missing columns in the train set\n",
    "    missing_categories = set(testSet) - set(trainSet)\n",
    "\n",
    "    # Add the missing columns in train set with default value equal to 0\n",
    "    for c in missing_categories:\n",
    "        trainSet[c] = 0\n",
    "\n",
    "    # Ensure the order of columns in the test and train sets are the same\n",
    "    trainSet, testSet = trainSet.align(testSet, axis=1)\n",
    "    \n",
    "    # We return the two set\n",
    "    return trainSet, testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separons les variables explicatives qualitatives de celles quantitatives\n",
    "df_train_quanti = df_train.drop(['Name','Platform','Genre','Publisher','Developer','Rating'], axis=1)\n",
    "df_test_quanti = df_test.drop(['Platform','Genre','Publisher','Developer','Rating'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut ici enlever les variables quantitatives non probantes\n",
    "#df_train_quanti = df_train_quanti.drop(['Year_of_Release'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['JP_Sales'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['Other_Sales'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['Critic_Score'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['Critic_Count'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['User_Score'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['User_Count'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['NA_Sales'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['Global_Sales'], axis=1)\n",
    "\n",
    "#df_test_quanti = df_test_quanti.drop(['Year_of_Release'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['JP_Sales'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['Other_Sales'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['Critic_Score'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['Critic_Count'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['User_Score'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['User_Count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories (train) : 30\n",
      "Nbr. of categories (test) : 27\n"
     ]
    }
   ],
   "source": [
    "# Variable Qualitative #1 : Platform\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Platform.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Platform.nunique()))\n",
    "\n",
    "df_train_quanti, df_test_quanti = binerizedQualitativeVariable(df_train_quanti, df_test_quanti, df_train.Platform, df_test.Platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories (train) : 12\n",
      "Nbr. of categories (test) : 12\n"
     ]
    }
   ],
   "source": [
    "# Variable Qualitative #2 : Genre\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Genre.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Genre.nunique()))\n",
    "\n",
    "df_train_quanti, df_test_quanti = binerizedQualitativeVariable(df_train_quanti, df_test_quanti, df_train.Genre, df_test.Genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories (train) : 552\n",
      "Nbr. of categories (test) : 280\n"
     ]
    }
   ],
   "source": [
    "# Variable Qualitative #3 : Publisher\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Publisher.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Publisher.nunique()))\n",
    "\n",
    "df_train_quanti, df_test_quanti = binerizedQualitativeVariable(df_train_quanti, df_test_quanti, df_train.Publisher, df_test.Publisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories (train) : 1593\n",
      "Nbr. of categories (test) : 677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nBon ici on a un probleme, si on ajoute les colonnes binaires generees par cette variable on ajoute 100mb\\na notre csv. Ceci rend peu pratique le prototypage, pour l'instant on le laisse tomber. On essayera de ce\\ndonner une raison rationnelle de le domper plus tard dans l'analyse. Hypothese, beaucoup de colinearite avec \\nle Publisher.\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable Qualitative #4 : Developer\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Developer.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Developer.nunique()))\n",
    "\n",
    "\"\"\"\n",
    "Bon ici on a un probleme, si on ajoute les colonnes binaires generees par cette variable on ajoute 100mb\n",
    "a notre csv. Ceci rend peu pratique le prototypage, pour l'instant on le laisse tomber. On essayera de ce\n",
    "donner une raison rationnelle de le domper plus tard dans l'analyse. Hypothese, beaucoup de colinearite avec \n",
    "le Publisher.\n",
    "\"\"\"\n",
    "#df_train_quanti, df_test_quanti = binerizedQualitativeVariable(df_train_quanti, df_test_quanti, df_train.Developer, df_test.Developer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories (train) : 8\n",
      "Nbr. of categories (test) : 5\n"
     ]
    }
   ],
   "source": [
    "# Variable Qualitative #5 : Rating\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Rating.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Rating.nunique()))\n",
    "\n",
    "df_train_quanti, df_test_quanti = binerizedQualitativeVariable(df_train_quanti, df_test_quanti, df_train.Rating, df_test.Rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de NaN sur 14094 observations \n",
      "\n",
      "Critic_Count : 7186\n",
      "Critic_Score : 7186\n",
      "User_Count : 7653\n",
      "User_Score : 7653\n",
      "Year_of_Release : 232\n"
     ]
    }
   ],
   "source": [
    "# On regarde le nombre de NaN par variables explicatives\n",
    "print(\"Nombre de NaN sur %d observations \\n\" % (len(df_train_quanti)))\n",
    "\n",
    "counter = 0;\n",
    "for i in df_train_quanti.isna().sum():\n",
    "    if(i > 0):\n",
    "        print(\"%s : %d\" % (df_train_quanti.columns[counter],i))\n",
    "        \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apres clean-up, nombre d'observation : 5857\n"
     ]
    }
   ],
   "source": [
    "# On garde seulement les observations ou nous avons toutes les donnees (a ameliorer plus tard)\n",
    "df_train_quanti = df_train_quanti.dropna()\n",
    "\n",
    "# On calcule le nombre d'observations restantes\n",
    "print(\"\\nApres clean-up, nombre d'observation : %d\" % (len(df_train_quanti)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enleve les colonnes qui ne comportent que des zeros\n",
    "cols_only0 = (df_train_quanti != 0).any(axis=0)\n",
    "df_train_quanti = df_train_quanti.loc[:,cols_only0]\n",
    "df_test_quanti = df_test_quanti.loc[:,cols_only0]\n",
    "\n",
    "# On enleve toutes les colonnes qui sont en double\n",
    "duplicates = df_train_quanti.T.duplicated()\n",
    "duplicates = ~duplicates\n",
    "df_train_quanti = df_train_quanti.T[duplicates].T\n",
    "df_test_quanti = df_test_quanti.T[duplicates].T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sauvegarde nos deux nouveaux jeux de donnees entierement quantitatif\n",
    "#df_train_quanti.to_csv(\"train_quanti.csv\")\n",
    "#df_test_quanti.to_csv(\"test_quanti.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(y,x):\n",
    "    \n",
    "    # Number of Explicative Variables\n",
    "    k = 0\n",
    "    try:\n",
    "        k = x.shape[1]\n",
    "    except IndexError:\n",
    "        k = 1\n",
    "        \n",
    "    \n",
    "    # The variance-covariance\n",
    "    C = np.linalg.inv(np.dot(x.T,x))\n",
    "    \n",
    "    B = np.dot(np.dot(C,x.T),y)\n",
    "    \n",
    "    return B, np.dot(x,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R2_adj(Y,W,p):\n",
    "\n",
    "    SS_tot = 0.0\n",
    "    SS_reg = 0.0\n",
    "    SS_res = 0.0    \n",
    "    \n",
    "    y_S = np.sum(Y)/len(Y)\n",
    "\n",
    "    for i in range(len(Y)):\n",
    "        SS_tot += (Y[i] - y_S)**2\n",
    "        SS_reg += (W[i] - y_S)**2\n",
    "        SS_res += (Y[i] - W[i])**2\n",
    "        \n",
    "    \n",
    "    n = len(Y)\n",
    "    \n",
    "    Radj = 1 - ((SS_res)/float(n-p))/((SS_tot)/(n-1))\n",
    "\n",
    "    return Radj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ici on prend en input la variable d'interet et la matrice des variables explicatives. Avec une analyse en \n",
    "composante principale on obtient de nouveau coefficient pour notre regression lineaire.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def pca(Y,X):\n",
    "    \n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # The covariance matrix\n",
    "    E = np.dot(X_std.T,X_std)/(nbrOfVar-1)\n",
    "\n",
    "    # Eigenvalues and Eigenvectors\n",
    "    eig_val, eig_vec = np.linalg.eig(E) #vector already normalized\n",
    "\n",
    "    eig_val = eig_val.real # Remove imaginary part\n",
    "\n",
    "    eig_val_contribution = eig_val/np.sum(eig_val) # Extract the explained variable for each eigenvalues\n",
    "    \n",
    "    rangeIndex = np.array(range(len(eig_val_contribution)))\n",
    "    eig_val_contribution = np.array(([rangeIndex,eig_val_contribution])).T # Concatenate an index column\n",
    "    \n",
    "    eig_val_contribution = eig_val_contribution[(-eig_val_contribution[:,1]).argsort()] # Sort in descending order for col[1]\n",
    "\n",
    "    #print(\"Explained variance : \")\n",
    "    sumSofar = 0.0\n",
    "    stopIndex = 0\n",
    "    for i in range(len(eig_val)):\n",
    "        sumSofar += eig_val_contribution[i,1]\n",
    "        #print(\"%d. Eig %d = %.4f --- sum : %.4f\" % (i+1,eig_val_contribution[i,0],eig_val_contribution[i,1],sumSofar))\n",
    "        if(sumSofar >= 0.9999):\n",
    "            stopIndex = i\n",
    "            break\n",
    "\n",
    "    indexKept = eig_val_contribution[:stopIndex,0]\n",
    "\n",
    "    # Our reduced regressor\n",
    "    K_vect = eig_vec[indexKept.astype(int)].T.real\n",
    "\n",
    "    # Our principal components\n",
    "    Z = np.dot(X_std,K_vect)\n",
    "\n",
    "    # Our Regression Coefficiants\n",
    "    B_partial = np.dot(np.linalg.inv(np.dot(Z.T,Z)),Z.T)\n",
    "    B = np.dot(B_partial,Y)\n",
    "    \n",
    "    # We calculate the intercept\n",
    "    uY = np.mean(Y)\n",
    "\n",
    "    # We add the intercept\n",
    "    Y_new = np.dot(Z,B) + uY\n",
    "    \n",
    "    # Clip to min 0\n",
    "    Y_new = Y_new.clip(min=0)\n",
    "\n",
    "    # New R2 and RMSE\n",
    "    try:\n",
    "        RMSE(Y,Y_new)\n",
    "        print(\"R2_adj : %.2f\" %(R2_adj(Y,Y_new,nbrOfVar)))\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"NUL, hahaha determinant = 0\")\n",
    "        \n",
    "    return Y_new, B, K_vect, uY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_trained(X,B,K_vect,uY):\n",
    "    \n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Our principal components\n",
    "    Z = np.dot(X_std,K_vect)\n",
    "    \n",
    "    # We add the intercept\n",
    "    Y_new = np.dot(Z,B) + uY\n",
    "    \n",
    "    # Clip to min 0\n",
    "    Y_new = Y_new.clip(min=0)\n",
    "\n",
    "    return Y_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On definit notre variable d'interet\n",
    "Y = np.array(df_train_quanti.NA_Sales)\n",
    "\n",
    "# On definit notre vecteur de variables explicatives\n",
    "X = np.array(df_train_quanti.drop(['NA_Sales','Global_Sales'], axis=1))\n",
    "nbrOfVar = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Shape :  (5857,)\n",
      "X Shape :  (5857, 294)\n",
      "Nbr de Variables :  294\n"
     ]
    }
   ],
   "source": [
    "# On s'assure que nos matrices soient toutes dans la bonne position\n",
    "print(\"Y Shape : \",Y.shape)\n",
    "print(\"X Shape : \",X.shape)\n",
    "print(\"Nbr de Variables : \",nbrOfVar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value = 0.39, RMSE = 0.36\n",
      "R2_adj : 0.60\n"
     ]
    }
   ],
   "source": [
    "# We train our model\n",
    "Y_new, B, K_vect, uY = pca(Y,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de NaN sur 2490 observations \n",
      "\n",
      "Critic_Count : 1292\n",
      "Critic_Score : 1292\n",
      "User_Count : 1370\n",
      "User_Score : 1370\n",
      "Year_of_Release : 35\n"
     ]
    }
   ],
   "source": [
    "# On regarde le nombre de NaN par variables explicatives\n",
    "print(\"Nombre de NaN sur %d observations \\n\" % (len(df_test_quanti)))\n",
    "\n",
    "counter = 0;\n",
    "for i in df_test_quanti.isna().sum():\n",
    "    if(i > 0):\n",
    "        print(\"%s : %d\" % (df_test_quanti.columns[counter],i))\n",
    "        \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateColumn(trainSet,testSet,colName):\n",
    "    \n",
    "    # On definit notre variable d'interet\n",
    "    Y = np.array(trainSet[colName])\n",
    "\n",
    "    # On definit notre vecteur de variables explicatives\n",
    "    X = np.array(trainSet.drop([colName], axis=1))\n",
    "    nbrOfVar = X.shape[1]\n",
    "\n",
    "    # We train our model\n",
    "    [Y_new, B, K_vect, uY] = pca(Y,X)    \n",
    "    \n",
    "    nbrOfEstimate = 0\n",
    "    testSet_minusCol = testSet.drop([colName], axis=1)\n",
    "    for i, row in testSet.iterrows():\n",
    "        if(np.isnan(testSet.loc[i,colName])):\n",
    "            X = np.array(testSet_minusCol.iloc[1])\n",
    "            X = X.reshape(X.shape[0],1).T\n",
    "            y = pca_trained(X,B,K_vect,uY)\n",
    "            y = y[0]\n",
    "            testSet.loc[i,colName] = y \n",
    "            nbrOfEstimate +=1\n",
    "    \n",
    "    print(nbrOfEstimate)\n",
    "    \n",
    "    return testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value = 28.81, RMSE = 200.90\n",
      "R2_adj : 0.43\n",
      "1292\n"
     ]
    }
   ],
   "source": [
    "trainSet = df_train_quanti.drop(['Critic_Score','User_Count','User_Score','Year_of_Release'], axis=1)\n",
    "testSet = df_test_quanti.drop(['Critic_Score','User_Count','User_Score','Year_of_Release'], axis=1)\n",
    "\n",
    "df_test_quanti.Critic_Count = estimateColumn(trainSet,testSet,'Critic_Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value = 70.30, RMSE = 120.53\n",
      "R2_adj : 0.35\n",
      "1292\n"
     ]
    }
   ],
   "source": [
    "trainSet = df_train_quanti.drop(['User_Count','User_Score','Year_of_Release'], axis=1)\n",
    "testSet = df_test_quanti.drop(['User_Count','User_Score','Year_of_Release'], axis=1)\n",
    "df_test_quanti.Critic_Score = estimateColumn(trainSet,testSet,'Critic_Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value = 169.13, RMSE = 185298.89\n",
      "R2_adj : 0.36\n",
      "1370\n"
     ]
    }
   ],
   "source": [
    "trainSet = df_train_quanti.drop(['User_Score','Year_of_Release'], axis=1)\n",
    "testSet = df_test_quanti.drop(['User_Score','Year_of_Release'], axis=1)\n",
    "df_test_quanti.User_Count = estimateColumn(trainSet,testSet,'User_Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value = 7.20, RMSE = 1.05\n",
      "R2_adj : 0.46\n",
      "1370\n"
     ]
    }
   ],
   "source": [
    "trainSet = df_train_quanti.drop(['Year_of_Release'], axis=1)\n",
    "testSet = df_test_quanti.drop(['Year_of_Release'], axis=1)\n",
    "df_test_quanti.User_Score = estimateColumn(trainSet,testSet,'User_Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value = 2007.44, RMSE = 3.64\n",
      "R2_adj : 0.79\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "trainSet = df_train_quanti\n",
    "testSet = df_test_quanti\n",
    "df_test_quanti.Year_of_Release = estimateColumn(trainSet,testSet,'Year_of_Release')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de NaN sur 2490 observations \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On regarde le nombre de NaN par variables explicatives\n",
    "print(\"Nombre de NaN sur %d observations \\n\" % (len(df_test_quanti)))\n",
    "\n",
    "counter = 0;\n",
    "for i in df_test_quanti.isna().sum():\n",
    "    if(i > 0):\n",
    "        print(\"%s : %d\" % (df_test_quanti.columns[counter],i))\n",
    "        \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addColumn(trainSet,testSet,colName):\n",
    "    \n",
    "    # On definit notre variable d'interet\n",
    "    Y = np.array(trainSet[colName])\n",
    "\n",
    "    # On definit notre vecteur de variables explicatives\n",
    "    X = np.array(trainSet.drop([colName], axis=1))\n",
    "\n",
    "    # We train our model\n",
    "    [Y_new, B, K_vect, uY] = pca(Y,X)\n",
    "    \n",
    "    \n",
    "    X_test = np.array(testSet.drop([colName], axis=1))    \n",
    "    Y_test = pca_trained(X_test,B,K_vect,uY)\n",
    "    \n",
    "    return Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value = 0.39, RMSE = 0.04\n",
      "R2_adj : 0.95\n"
     ]
    }
   ],
   "source": [
    "df_test_quanti['NA_Sales'] = addColumn(df_train_quanti,df_test_quanti,'NA_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value = 0.78, RMSE = 0.12\n",
      "R2_adj : 0.97\n"
     ]
    }
   ],
   "source": [
    "df_test_quanti['Global_Sales'] = addColumn(df_train_quanti,df_test_quanti,'Global_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final = df_test_quanti[['Global_Sales','NA_Sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final.to_csv(\"test_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
