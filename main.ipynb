{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe les librairies utiles\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# Chemin vers les csv\n",
    "pathTest = \"./test3.csv\"\n",
    "pathTrain = \"./train3.csv\"\n",
    "\n",
    "# Importe les jeux de donnees\n",
    "df_test = pd.read_csv(pathTest, sep=',', index_col=0)\n",
    "df_train = pd.read_csv(pathTrain, sep=',', index_col=0)\n",
    "\n",
    "# On enleve tout de suite la colonne Name de notre jeu de donnees train\n",
    "df_train = df_train.drop(['Name'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# --- Variables Explicatives ---\n",
    "\n",
    "#    Name : Nom du jeu\n",
    "#    Platform : Console sur laquelle le jeu fonctionne\n",
    "#    Year_of_Release : Année de sortie du jeu\n",
    "#    Genre\n",
    "#    Publisher\n",
    "#    JP_Sales : Nombre de ventes du jeu au Japon en millions d’unités\n",
    "#    Other_Sales : Nombre de ventes du jeu ailleurs dans le monde : Afrique, Asie sans le Japon, Europe sans l’Union Européenne et Amérique du Sud en millions d’unités\n",
    "#    Critic_Score : Score donné par Metacritic\n",
    "#    Critic_Count : Nombre de critiques prises en compte pour estimer le Critic_score\n",
    "#    User_Score : Score donné par les usagers de Metacritic\n",
    "#    User_Count : Nombre d’usagers considérés pour estimer le User_Score\n",
    "#    Developer : Compagnie créatrice du jeu\n",
    "#    Rating : Classement ESRB (Entertainment Software Rating Board) ie à qui s’addresse le jeu (tout public, majeur, adolescents, etc) \n",
    "\n",
    "\n",
    "# --- Variables d'Interet ---\n",
    "\n",
    "#    NA_sales : Nombre de ventes du jeu en Amérique du Nord en millions d’unités\n",
    "#    Global_Sales : Nombre de ventes total du jeu en millions d’unités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--- Evaluation des predictions --- \n",
    "\n",
    "Dans le document MTH3302_CriteresProjet-1.pdf on nous informe que la precision\n",
    "de nos estimations sera evaluee avec le root mean square error (RMSE)\n",
    "\n",
    "On definit cette fonction ci-bas,\n",
    "\n",
    "    Y : Variable d'interet\n",
    "    W : Predictions de la variable d'interet\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def RMSE(Y,W):\n",
    "    \n",
    "    # Nombre d'observations\n",
    "    n = len(Y)\n",
    "    \n",
    "    total = 0.0\n",
    "    for i in range(n):\n",
    "        total += (Y[i] - W[i])**2\n",
    "    \n",
    "    RMSE = total/float(n)\n",
    "    \n",
    "    return RMSE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--- Variable Qualitative -> Table Binaire --- \n",
    "\n",
    "Afin de traiter nos variables qualitatives, nous avons une fonction qui la tranforme en table binaire \n",
    "avec un one bit encoder. \n",
    "\n",
    "    Disons que la variable explicative peut prendre 4 valeurs {Bleu, Vert, Jaune, Rouge},\n",
    "        \n",
    "          ID        X1\n",
    "        ----------------\n",
    "          1        Bleu\n",
    "          2        Bleu\n",
    "          3        Vert\n",
    "          4        Jaune\n",
    "          5        Vert\n",
    "          6        Rouge          \n",
    "         ...\n",
    "    \n",
    "    Suite a un encodage en 1 bit notre variable explicative X1 devient,\n",
    "    \n",
    "          ID        Bleu        Vert        Jaune        Rouge\n",
    "        -------------------------------------------------------\n",
    "          1          1           0            0            0\n",
    "          2          1           0            0            0\n",
    "          3          0           1            0            0\n",
    "          4          0           0            1            0\n",
    "          5          0           1            0            0\n",
    "          6          0           0            0            1\n",
    "                                ...\n",
    "                                \n",
    "\n",
    "Nous nous basons sur la fonction get_dummies() de la librairie Panda pour faire ceci.\n",
    "\n",
    "\n",
    "\n",
    "Input,\n",
    "\n",
    "    trainSet : le jeu de donnees de training\n",
    "    testSet : le jeu de donnees de test\n",
    "    varName : la variable qualitative que nous souhaitons transformer en tableau binaire\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def qualiToBinary(trainSet, testSet, varName):\n",
    "    \n",
    "    # Genere la table binaire de la variable qualitative\n",
    "    trainVar_binary = pd.get_dummies(trainSet[varName])\n",
    "    testVar_binary = pd.get_dummies(testSet[varName])\n",
    "    \n",
    "    # On enleve la variable qualitative de nos jeux de donnees\n",
    "    trainSet = trainSet.drop([varName], axis=1)\n",
    "    testSet = testSet.drop([varName], axis=1)\n",
    "    \n",
    "    # On convertit nos tableaux binaires en Dataframe\n",
    "    trainVar_binary = pd.DataFrame(trainVar_binary)\n",
    "    testVar_binary = pd.DataFrame(testVar_binary)\n",
    "\n",
    "    # On ajoute les nouvelles colonnes a nos jeux de donnees\n",
    "    trainSet = pd.concat([trainSet,trainVar_binary],axis=1)\n",
    "    testSet = pd.concat([testSet,testVar_binary],axis=1)\n",
    "    \n",
    "    # On retourne les jeux de donnees\n",
    "    return trainSet, testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut ici enlever les variables quantitatives non probantes\n",
    "#df_train_quanti = df_train_quanti.drop(['Year_of_Release'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['JP_Sales'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['Other_Sales'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['Critic_Score'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['Critic_Count'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['User_Score'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['User_Count'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['NA_Sales'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['Global_Sales'], axis=1)\n",
    "\n",
    "#df_test_quanti = df_test_quanti.drop(['Year_of_Release'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['JP_Sales'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['Other_Sales'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['Critic_Score'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['Critic_Count'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['User_Score'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['User_Count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories (train) : 30\n",
      "Nbr. of categories (test) : 27\n"
     ]
    }
   ],
   "source": [
    "# Variable Qualitative #1 : Platform\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Platform.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Platform.nunique()))\n",
    "\n",
    "df_train, df_test = qualiToBinary(df_train, df_test, 'Platform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories (train) : 12\n",
      "Nbr. of categories (test) : 12\n"
     ]
    }
   ],
   "source": [
    "# Variable Qualitative #2 : Genre\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Genre.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Genre.nunique()))\n",
    "\n",
    "df_train, df_test = qualiToBinary(df_train, df_test,'Genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories (train) : 552\n",
      "Nbr. of categories (test) : 280\n"
     ]
    }
   ],
   "source": [
    "# Variable Qualitative #3 : Publisher\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Publisher.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Publisher.nunique()))\n",
    "\n",
    "df_train, df_test = qualiToBinary(df_train, df_test, 'Publisher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories (train) : 1593\n",
      "Nbr. of categories (test) : 677\n"
     ]
    }
   ],
   "source": [
    "# Variable Qualitative #4 : Developer\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Developer.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Developer.nunique()))\n",
    "\n",
    "\n",
    "# Bon ici on a un probleme, si on ajoute les colonnes binaires generees par cette variable on ajoute 100mb\n",
    "# a notre csv. Ceci rend peu pratique le prototypage, pour l'instant on le laisse tomber. On essayera de ce\n",
    "# donner une raison rationnelle de le domper plus tard dans l'analyse. Hypothese, beaucoup de colinearite avec \n",
    "# le Publisher.\n",
    "\n",
    "#df_train, df_test = qualiToBinary(df_train, df_test,'Developer')\n",
    "df_test = df_test.drop(['Developer'],axis=1)\n",
    "df_train = df_train.drop(['Developer'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories (train) : 8\n",
      "Nbr. of categories (test) : 5\n"
     ]
    }
   ],
   "source": [
    "# Variable Qualitative #5 : Rating\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Rating.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Rating.nunique()))\n",
    "\n",
    "df_train, df_test = qualiToBinary(df_train, df_test,'Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On identifie les colonnes manquantes dans le jeu de donnees test\n",
    "missing_categories = set(df_train) - set(df_test)\n",
    "\n",
    "# On ajoute ces colonnes manquantes avec une valeur par defaut de 0\n",
    "for c in missing_categories:\n",
    "    df_test[c] = 0\n",
    "\n",
    "# On identifie les colonnes manquantes dans le jeu de donnees training\n",
    "missing_categories = set(df_test) - set(df_train)\n",
    "\n",
    "# On ajoute ces colonnes manquantes avec une valeur par defaut de 0\n",
    "for c in missing_categories:\n",
    "    df_train[c] = 0\n",
    "\n",
    "\n",
    "# On aligne nos jeux de donnees pour que les memes colonnes soient aux memes endroits\n",
    "df_train, df_test = df_train.align(df_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de NaN sur 14094 observations \n",
      "\n",
      "Critic_Count : 7186\n",
      "Critic_Score : 7186\n",
      "User_Count : 7653\n",
      "User_Score : 7653\n",
      "Year_of_Release : 232\n"
     ]
    }
   ],
   "source": [
    "# On regarde le nombre de NaN par variables explicatives\n",
    "print(\"Nombre de NaN sur %d observations \\n\" % (len(df_train)))\n",
    "\n",
    "counter = 0;\n",
    "for i in df_train.isna().sum():\n",
    "    if(i > 0):\n",
    "        print(\"%s : %d\" % (df_train.columns[counter],i))\n",
    "        \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apres clean-up, nombre d'observation : 5857\n"
     ]
    }
   ],
   "source": [
    "# On garde seulement les observations ou nous avons toutes les donnees (a ameliorer plus tard)\n",
    "df_train = df_train.dropna()\n",
    "\n",
    "# On calcule le nombre d'observations restantes\n",
    "print(\"\\nApres clean-up, nombre d'observation : %d\" % (len(df_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nombre de colonne: 641\n",
      "\n",
      "Nombre de colonne apres clean up : 296\n"
     ]
    }
   ],
   "source": [
    "# On calcule le nombre de colonne\n",
    "print(\"\\nNombre de colonne: %d\" % (len(df_train.T)))\n",
    "\n",
    "# On enleve les colonnes qui ne comportent que des zeros\n",
    "cols_only0 = (df_train != 0).any(axis=0)\n",
    "df_train = df_train.loc[:,cols_only0]\n",
    "df_test = df_test.loc[:,cols_only0]\n",
    "\n",
    "# On enleve toutes les colonnes qui sont en double\n",
    "duplicates = df_train.T.duplicated()\n",
    "duplicates = ~duplicates\n",
    "df_train = df_train.T[duplicates].T\n",
    "df_test = df_test.T[duplicates].T\n",
    "\n",
    "# On calcule le nombre de colonne\n",
    "print(\"\\nNombre de colonne apres clean up : %d\" % (len(df_train.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sauvegarde nos deux nouveaux jeux de donnees entierement quantitatif\n",
    "#df_train.to_csv(\"train_clean.csv\")\n",
    "#df_test.to_csv(\"test_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(y,x):\n",
    "    \n",
    "    # Number of Explicative Variables\n",
    "    k = 0\n",
    "    try:\n",
    "        k = x.shape[1]\n",
    "    except IndexError:\n",
    "        k = 1\n",
    "        \n",
    "    \n",
    "    # The variance-covariance\n",
    "    C = np.linalg.inv(np.dot(x.T,x))\n",
    "    \n",
    "    B = np.dot(np.dot(C,x.T),y)\n",
    "    \n",
    "    return B, np.dot(x,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R2_adj(Y,W,p):\n",
    "\n",
    "    SS_tot = 0.0\n",
    "    SS_reg = 0.0\n",
    "    SS_res = 0.0    \n",
    "    \n",
    "    y_S = np.sum(Y)/len(Y)\n",
    "\n",
    "    for i in range(len(Y)):\n",
    "        SS_tot += (Y[i] - y_S)**2\n",
    "        SS_reg += (W[i] - y_S)**2\n",
    "        SS_res += (Y[i] - W[i])**2\n",
    "        \n",
    "    \n",
    "    n = len(Y)\n",
    "    \n",
    "    Radj = 1 - ((SS_res)/float(n-p))/((SS_tot)/(n-1))\n",
    "\n",
    "    return Radj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(Y,X,varExplained):\n",
    "    \n",
    "    # On transforme X en une matrice de moyenne = 0 et variance = 1 \n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "    nbrOfVar = X.shape[1]\n",
    "\n",
    "    # On calcule la matrice de covariance\n",
    "    E = np.dot(X_std.T,X_std)/(nbrOfVar-1)\n",
    "\n",
    "    # On calcule nos Valeurs et Vecteurs propres\n",
    "    eig_val, eig_vec = np.linalg.eig(E)\n",
    "    eig_val = eig_val.real\n",
    "    eig_vec = eig_vec.real\n",
    "    eig_vec = eig_vec.T # pour les ordonner par range\n",
    "\n",
    "    # On calcule la contribution de chaque valeur propre a la variance total\n",
    "    eig_val_contribution = eig_val/np.sum(eig_val)\n",
    "\n",
    "    # On cree une liste d'index de 0 au nombre de valeurs propres\n",
    "    rangeIndex = np.array(range(len(eig_val_contribution)))\n",
    "\n",
    "    # On cree un tableau indexe de nos valeurs propres et leur contribution a la variance\n",
    "    eig_val_table = np.array(([rangeIndex,eig_val,eig_val_contribution])).T\n",
    "\n",
    "    # On ordonne en ordre decroissant de contribution le tableau\n",
    "    eig_val_table_ordered = eig_val_table[(-eig_val_table[:,2]).argsort()]\n",
    "    \n",
    "    # On rejoint la quantite de variance souhaitee\n",
    "    varCumul = 0.0\n",
    "    varIndex = 0\n",
    "    for val in eig_val_table_ordered:\n",
    "        varIndex += 1\n",
    "        varCumul += val[2]\n",
    "        if(varCumul >= varExplained):\n",
    "            break\n",
    "    \n",
    "    # On recupere les index des valeurs propres que nous garderont\n",
    "    indexKept = eig_val_table_ordered[:,0][:varIndex]\n",
    "\n",
    "    # Our reduced regressor\n",
    "    K_vect = eig_vec[indexKept.astype(int),:].T\n",
    "\n",
    "    # Our principal components\n",
    "    Z = np.dot(X_std,K_vect)\n",
    "\n",
    "    # Our Regression Coefficiants\n",
    "    B_partial = np.dot(np.linalg.inv(np.dot(Z.T,Z)),Z.T)\n",
    "    B = np.dot(B_partial,Y)\n",
    "\n",
    "    # We calculate the intercept\n",
    "    uY = np.mean(Y)\n",
    "\n",
    "    # We add the intercept\n",
    "    Y_new = np.dot(Z,B) + uY\n",
    "\n",
    "    # Clip to min 0\n",
    "    Y_new = Y_new.clip(min=0)\n",
    "\n",
    "    print(\"R2 : %.4f\" % (R2_adj(Y,Y_new,varIndex)))\n",
    "    print(\"RMSE = %.4f\" % (RMSE(Y,Y_new)))\n",
    "    \n",
    "    return Y_new, B, K_vect, uY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_trained(X,B,K_vect,uY):\n",
    "    \n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Our principal components\n",
    "    Z = np.dot(X_std,K_vect)\n",
    "    \n",
    "    # We add the intercept\n",
    "    Y_new = np.dot(Z,B) + uY\n",
    "    \n",
    "    # Clip to min 0\n",
    "    Y_new = Y_new.clip(min=0)\n",
    "\n",
    "    return Y_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.fillna(df_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de NaN sur 2490 observations \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On regarde le nombre de NaN par variables explicatives\n",
    "print(\"Nombre de NaN sur %d observations \\n\" % (len(df_test)))\n",
    "\n",
    "counter = 0;\n",
    "for i in df_test.isna().sum():\n",
    "    if(i > 0):\n",
    "        print(\"%s : %d\" % (df_test.columns[counter],i))\n",
    "        \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addColumn(trainSet,testSet,colName):\n",
    "    \n",
    "    # On definit notre variable d'interet\n",
    "    Y = np.array(trainSet[colName])\n",
    "\n",
    "    # On definit notre vecteur de variables explicatives\n",
    "    X = np.array(trainSet.drop([colName], axis=1))\n",
    "\n",
    "    # We train our model\n",
    "    [Y_new, B, K_vect, uY] = PCA(Y,X,0.9)\n",
    "    \n",
    "    X_test = np.array(testSet.drop([colName], axis=1))    \n",
    "    Y_test = PCA_trained(X_test,B,K_vect,uY)\n",
    "    \n",
    "    return Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 : 0.7788\n",
      "RMSE = 0.2038\n"
     ]
    }
   ],
   "source": [
    "df_test['NA_Sales'] = addColumn(df_train,df_test,'NA_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 : 0.9271\n",
      "RMSE = 0.2730\n"
     ]
    }
   ],
   "source": [
    "df_test['Global_Sales'] = addColumn(df_train,df_test,'Global_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final = df_test[['Global_Sales','NA_Sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final.to_csv(\"test_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
