{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe les librairies utiles\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# Chemin vers les csv\n",
    "pathTest = \"./test3.csv\"\n",
    "pathTrain = \"./train3.csv\"\n",
    "\n",
    "# Importe les jeux de donnees\n",
    "df_test = pd.read_csv(pathTest, sep=',', index_col=0)\n",
    "df_train = pd.read_csv(pathTrain, sep=',', index_col=0)\n",
    "\n",
    "# On enleve tout de suite la colonne Name de notre jeu de donnees train\n",
    "df_train = df_train.drop(['Name'], axis=1)\n",
    "\n",
    "df_train = df_train.fillna(df_train.median())\n",
    "df_test = df_test.fillna(df_test.median())\n",
    "# --- Variables Explicatives ---\n",
    "\n",
    "#    Name : Nom du jeu\n",
    "#    Platform : Console sur laquelle le jeu fonctionne\n",
    "#    Year_of_Release : Année de sortie du jeu\n",
    "#    Genre\n",
    "#    Publisher\n",
    "#    JP_Sales : Nombre de ventes du jeu au Japon en millions d’unités\n",
    "#    Other_Sales : Nombre de ventes du jeu ailleurs dans le monde : Afrique, Asie sans le Japon, Europe sans l’Union Européenne et Amérique du Sud en millions d’unités\n",
    "#    Critic_Score : Score donné par Metacritic\n",
    "#    Critic_Count : Nombre de critiques prises en compte pour estimer le Critic_score\n",
    "#    User_Score : Score donné par les usagers de Metacritic\n",
    "#    User_Count : Nombre d’usagers considérés pour estimer le User_Score\n",
    "#    Developer : Compagnie créatrice du jeu\n",
    "#    Rating : Classement ESRB (Entertainment Software Rating Board) ie à qui s’addresse le jeu (tout public, majeur, adolescents, etc) \n",
    "\n",
    "\n",
    "# --- Variables d'Interet ---\n",
    "\n",
    "#    NA_sales : Nombre de ventes du jeu en Amérique du Nord en millions d’unités\n",
    "#    Global_Sales : Nombre de ventes total du jeu en millions d’unités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--- Evaluation des predictions --- \n",
    "\n",
    "Dans le document MTH3302_CriteresProjet-1.pdf on nous informe que la precision\n",
    "de nos estimations sera evaluee avec le root mean square error (RMSE)\n",
    "\n",
    "On definit cette fonction ci-bas,\n",
    "\n",
    "    Y : Variable d'interet\n",
    "    W : Predictions de la variable d'interet\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def RMSE(Y,W):\n",
    "    \n",
    "    # Nombre d'observations\n",
    "    n = len(Y)\n",
    "    \n",
    "    total = 0.0\n",
    "    for i in range(n):\n",
    "        total += (Y[i] - W[i])**2\n",
    "    \n",
    "    RMSE = total/float(n)\n",
    "    \n",
    "    return RMSE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--- Variable Qualitative -> Table Binaire --- \n",
    "\n",
    "Afin de traiter nos variables qualitatives, nous avons une fonction qui la tranforme en table binaire \n",
    "avec un one bit encoder. \n",
    "\n",
    "    Disons que la variable explicative peut prendre 4 valeurs {Bleu, Vert, Jaune, Rouge},\n",
    "        \n",
    "          ID        X1\n",
    "        ----------------\n",
    "          1        Bleu\n",
    "          2        Bleu\n",
    "          3        Vert\n",
    "          4        Jaune\n",
    "          5        Vert\n",
    "          6        Rouge          \n",
    "         ...\n",
    "    \n",
    "    Suite a un encodage en 1 bit notre variable explicative X1 devient,\n",
    "    \n",
    "          ID        Bleu        Vert        Jaune        Rouge\n",
    "        -------------------------------------------------------\n",
    "          1          1           0            0            0\n",
    "          2          1           0            0            0\n",
    "          3          0           1            0            0\n",
    "          4          0           0            1            0\n",
    "          5          0           1            0            0\n",
    "          6          0           0            0            1\n",
    "                                ...\n",
    "                                \n",
    "\n",
    "Nous nous basons sur la fonction get_dummies() de la librairie Panda pour faire ceci.\n",
    "\n",
    "\n",
    "\n",
    "Input,\n",
    "\n",
    "    trainSet : le jeu de donnees de training\n",
    "    testSet : le jeu de donnees de test\n",
    "    varName : la variable qualitative que nous souhaitons transformer en tableau binaire\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def qualiToBinary(trainSet, testSet, varName):\n",
    "    \n",
    "    # Genere la table binaire de la variable qualitative\n",
    "    trainVar_binary = pd.get_dummies(trainSet[varName],prefix=varName)\n",
    "    testVar_binary = pd.get_dummies(testSet[varName],prefix=varName)\n",
    "    \n",
    "    # On enleve la variable qualitative de nos jeux de donnees\n",
    "    trainSet = trainSet.drop([varName], axis=1)\n",
    "    testSet = testSet.drop([varName], axis=1)\n",
    "    \n",
    "    # On convertit nos tableaux binaires en Dataframe\n",
    "    trainVar_binary = pd.DataFrame(trainVar_binary)\n",
    "    testVar_binary = pd.DataFrame(testVar_binary)\n",
    "\n",
    "    # On ajoute les nouvelles colonnes a nos jeux de donnees\n",
    "    trainSet = pd.concat([trainSet,trainVar_binary],axis=1)\n",
    "    testSet = pd.concat([testSet,testVar_binary],axis=1)\n",
    "    \n",
    "    # On identifie les colonnes manquantes dans le jeu de donnees test\n",
    "    missing_categories = set(testSet) - set(df_test)\n",
    "\n",
    "    # On ajoute ces colonnes manquantes avec une valeur par defaut de 0\n",
    "    for c in missing_categories:\n",
    "        testSet[c] = 0\n",
    "\n",
    "    # On identifie les colonnes manquantes dans le jeu de donnees training\n",
    "    missing_categories = set(testSet) - set(trainSet)\n",
    "\n",
    "    # On ajoute ces colonnes manquantes avec une valeur par defaut de 0\n",
    "    for c in missing_categories:\n",
    "        trainSet[c] = 0\n",
    "\n",
    "\n",
    "    # On aligne nos jeux de donnees pour que les memes colonnes soient aux memes endroits\n",
    "    trainSet, testSet = trainSet.align(testSet, axis=1)\n",
    "    \n",
    "    # On retourne les jeux de donnees\n",
    "    return trainSet, testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(y,x):\n",
    "    \n",
    "    # The variance-covariance\n",
    "    C = np.linalg.inv(np.dot(x.T,x))\n",
    "    \n",
    "    # On calcule les coefficients de regression\n",
    "    B = np.dot(np.dot(C,x.T),y)\n",
    "    \n",
    "    return B, np.dot(x,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R2_adj(Y,W,p):\n",
    "\n",
    "    SS_tot = 0.0\n",
    "    SS_reg = 0.0\n",
    "    SS_res = 0.0    \n",
    "    \n",
    "    y_S = np.sum(Y)/len(Y)\n",
    "\n",
    "    for i in range(len(Y)):\n",
    "        SS_tot += (Y[i] - y_S)**2\n",
    "        SS_reg += (W[i] - y_S)**2\n",
    "        SS_res += (Y[i] - W[i])**2\n",
    "        \n",
    "    \n",
    "    n = len(Y)\n",
    "    \n",
    "    Radj = 1 - ((SS_res)/float(n-p))/((SS_tot)/(n-1))\n",
    "    \n",
    "    print(\"R2_adj : %.2f\" % Radj)\n",
    "\n",
    "    return Radj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R2_prev(Y,W,X):\n",
    "    \n",
    "    # The variance-covariance\n",
    "    C = np.linalg.inv(np.dot(X.T,X))\n",
    "    \n",
    "    # Hat Matrix (X*B = H*Y)\n",
    "    H = np.dot(np.dot(X,C),X.T)\n",
    "    \n",
    "    # We are only interested in the (i,i) values\n",
    "    H = np.diagonal(H)\n",
    "    \n",
    "    # Mean of Y\n",
    "    y_S = np.sum(Y)/float(len(Y))\n",
    "    \n",
    "    num = 0.0\n",
    "    for i in range(len(Y)):\n",
    "        num += ((Y[i] - W[i])/float(1 - H[i]))**2\n",
    "\n",
    "    denom = 0.0\n",
    "    for i in range(len(Y)):\n",
    "        denom += (Y[i] - y_S)**2\n",
    "    \n",
    "    R2_prev = (1 - num/denom)\n",
    "    \n",
    "    return R2_prev\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printNaN(printSet):\n",
    "    # On regarde combien de NaN nous avons par colonne\n",
    "    for col in printSet:\n",
    "        if(printSet[col].isna().sum() > 0):\n",
    "            print('%d \\t %s' % (printSet[col].isna().sum(),col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut ici enlever les variables quantitatives non probantes\n",
    "#df_train_quanti = df_train_quanti.drop(['Year_of_Release'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['JP_Sales'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['Other_Sales'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['Critic_Score'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['Critic_Count'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['User_Score'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['User_Count'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['NA_Sales'], axis=1)\n",
    "#df_train_quanti = df_train_quanti.drop(['Global_Sales'], axis=1)\n",
    "\n",
    "#df_test_quanti = df_test_quanti.drop(['Year_of_Release'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['JP_Sales'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['Other_Sales'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['Critic_Score'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['Critic_Count'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['User_Score'], axis=1)\n",
    "#df_test_quanti = df_test_quanti.drop(['User_Count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories (train) : 30\n",
      "Nbr. of categories (test) : 27\n"
     ]
    }
   ],
   "source": [
    "# Variable Qualitative #1 : Platform\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Platform.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Platform.nunique()))\n",
    "\n",
    "df_train, df_test = qualiToBinary(df_train, df_test, 'Platform')\n",
    "#df_test = df_test.drop(['Platform'],axis=1)\n",
    "#df_train = df_train.drop(['Platform'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterVar(testSet,Y,colsName):\n",
    "    \n",
    "    # Only select columns start with 'colsName'\n",
    "    reducedSet = testSet.loc[:, testSet.columns.str.startswith(colsName)]\n",
    "    \n",
    "    # On calcule le nombre de colonne\n",
    "    nbrOfCols = len(reducedSet.T)\n",
    "    \n",
    "    # On cree une liste d'index de 0 au nombre de colonne\n",
    "    rangeIndex = np.array(range(nbrOfCols))\n",
    "    \n",
    "    sumWithX = np.zeros(nbrOfCols)\n",
    "    sumWithoutX = np.zeros(nbrOfCols)\n",
    "    names = np.array(['' for _ in range(nbrOfCols)], dtype=object)\n",
    "    index = 0\n",
    "    for col in reducedSet:\n",
    "        withX = reducedSet[col]*Y\n",
    "        withoutX = (1-reducedSet[col])*Y\n",
    "        sumWithX[index] = np.sum(withX)/(withX.astype(bool).sum(axis=0))\n",
    "        sumWithoutX[index] = np.sum(withoutX)/(withoutX.astype(bool).sum(axis=0))\n",
    "        names[index] += col\n",
    "        index += 1\n",
    "    \n",
    "    # On cree un tableau indexe de nos valeurs propres et leur contribution a la variance\n",
    "    aveY_table = np.array(([rangeIndex,sumWithX,sumWithoutX])).T\n",
    "    \n",
    "    for i in range(nbrOfCols):\n",
    "        print('%d. \\t %.3f \\t %.3f \\t %s ' % (aveY_table[i][0],aveY_table[i][1],aveY_table[i][2],names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. \t 0.705 \t 0.262 \t Platform_2600 \n",
      "1. \t 0.001 \t 0.266 \t Platform_3DO \n",
      "2. \t 0.171 \t 0.269 \t Platform_3DS \n",
      "3. \t 0.103 \t 0.266 \t Platform_DC \n",
      "4. \t 0.161 \t 0.281 \t Platform_DS \n",
      "5. \t 1.133 \t 0.260 \t Platform_GB \n",
      "6. \t 0.233 \t 0.267 \t Platform_GBA \n",
      "7. \t 0.236 \t 0.267 \t Platform_GC \n",
      "8. \t 0.843 \t 0.265 \t Platform_GEN \n",
      "9. \t nan \t 0.266 \t Platform_GG \n",
      "10. \t 0.440 \t 0.262 \t Platform_N64 \n",
      "11. \t 1.334 \t 0.259 \t Platform_NES \n",
      "12. \t 0.001 \t 0.266 \t Platform_NG \n",
      "13. \t 0.096 \t 0.276 \t Platform_PC \n",
      "14. \t 0.002 \t 0.266 \t Platform_PCFX \n",
      "15. \t 0.282 \t 0.264 \t Platform_PS \n",
      "16. \t 0.275 \t 0.264 \t Platform_PS2 \n",
      "17. \t 0.299 \t 0.263 \t Platform_PS3 \n",
      "18. \t 0.261 \t 0.266 \t Platform_PS4 \n",
      "19. \t 0.085 \t 0.280 \t Platform_PSP \n",
      "20. \t 0.033 \t 0.272 \t Platform_PSV \n",
      "21. \t 0.006 \t 0.268 \t Platform_SAT \n",
      "22. \t 0.167 \t 0.266 \t Platform_SCD \n",
      "23. \t 0.296 \t 0.265 \t Platform_SNES \n",
      "24. \t 0.001 \t 0.266 \t Platform_TG16 \n",
      "25. \t 0.001 \t 0.266 \t Platform_WS \n",
      "26. \t 0.397 \t 0.254 \t Platform_Wii \n",
      "27. \t 0.283 \t 0.265 \t Platform_WiiU \n",
      "28. \t 0.483 \t 0.248 \t Platform_X360 \n",
      "29. \t 0.233 \t 0.267 \t Platform_XB \n",
      "30. \t 0.365 \t 0.264 \t Platform_XOne \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jean-romain/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "Y = df_train.NA_Sales.values\n",
    "\n",
    "clusterVar(df_train,Y,'Platform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Qualitative #2 : Genre\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Genre.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Genre.nunique()))\n",
    "\n",
    "print(df_train.Genre.value_counts())\n",
    "\n",
    "df_train, df_test = qualiToBinary(df_train, df_test,'Genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Qualitative #3 : Publisher\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Publisher.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Publisher.nunique()))\n",
    "\n",
    "#df_train, df_test = qualiToBinary(df_train, df_test, 'Publisher')\n",
    "df_test = df_test.drop(['Publisher'],axis=1)\n",
    "df_train = df_train.drop(['Publisher'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Qualitative #4 : Developer\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Developer.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Developer.nunique()))\n",
    "\n",
    "\n",
    "# Bon ici on a un probleme, si on ajoute les colonnes binaires generees par cette variable on ajoute 100mb\n",
    "# a notre csv. Ceci rend peu pratique le prototypage, pour l'instant on le laisse tomber. On essayera de ce\n",
    "# donner une raison rationnelle de le domper plus tard dans l'analyse. Hypothese, beaucoup de colinearite avec \n",
    "# le Publisher.\n",
    "\n",
    "#df_train, df_test = qualiToBinary(df_train, df_test,'Developer')\n",
    "df_test = df_test.drop(['Developer'],axis=1)\n",
    "df_train = df_train.drop(['Developer'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Qualitative #5 : Rating\n",
    "print(\"Nbr. of categories (train) : %d\" %(df_train.Rating.nunique()))\n",
    "print(\"Nbr. of categories (test) : %d\" %(df_test.Rating.nunique()))\n",
    "\n",
    "df_train, df_test = qualiToBinary(df_train, df_test,'Rating')\n",
    "#df_test = df_test.drop(['Rating'],axis=1)\n",
    "#df_train = df_train.drop(['Rating'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule le nombre de colonne\n",
    "print(\"\\nNombre de colonne: %d\" % (len(df_train.T)))\n",
    "\n",
    "# On enleve les colonnes qui ne comportent que des zeros\n",
    "cols_only0 = (df_train != 0).any(axis=0)\n",
    "df_train = df_train.loc[:,cols_only0]\n",
    "df_test = df_test.loc[:,cols_only0]\n",
    "\n",
    "# On enleve toutes les colonnes qui sont en double\n",
    "duplicates = df_train.T.duplicated()\n",
    "duplicates = ~duplicates\n",
    "df_train = df_train.T[duplicates].T\n",
    "df_test = df_test.T[duplicates].T\n",
    "\n",
    "# On calcule le nombre de colonne\n",
    "print(\"\\nNombre de colonne apres clean up : %d\" % (len(df_train.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createX(testSet):\n",
    "    \n",
    "    X1 = testSet.JP_Sales.values\n",
    "    X2 = (testSet.Critic_Count.values**2)*(testSet.Critic_Score.values)\n",
    "    X3 = (testSet.Critic_Score.values)\n",
    "    X4 = testSet.Year_of_Release.values\n",
    "    X5 = testSet.Other_Sales.values\n",
    "    X6 = testSet.User_Score.values*testSet.User_Count.values**2\n",
    "    X7 = testSet.User_Count.values\n",
    "\n",
    "    X8_1 = testSet.Platform_PS2.values\n",
    "    X8_2 = testSet.Platform_DS.values\n",
    "    X8_3 = testSet.Platform_PS3.values\n",
    "    X8_4 = testSet.Platform_Wii.values\n",
    "    X8_5 = testSet.Platform_X360.values\n",
    "    X8_6 = testSet.Platform_PSP.values\n",
    "    X8_7 = testSet.Platform_PS.values\n",
    "    X8_8 = testSet.Platform_PC.values\n",
    "    X8_9 = testSet.Platform_XB.values\n",
    "    X8_10 = testSet.Platform_GBA.values\n",
    "    X8_11 = testSet.Platform_GC.values\n",
    "    X8_12 = testSet.Platform_3DS.values\n",
    "    X8_13 = testSet.Platform_PSV.values\n",
    "    X8_14 = testSet.Platform_PS4.values\n",
    "    X8_15 = testSet.Platform_N64.values\n",
    "    X8_16 = testSet.Platform_XOne.values\n",
    "    X8_17 = testSet.Platform_SNES.values\n",
    "    X8_18 = testSet.Platform_SAT.values\n",
    "    X8_19 = testSet.Platform_WiiU.values\n",
    "\n",
    "    X9_1 = testSet.Genre_Action.values\n",
    "    X9_2 = testSet.Genre_Sports.values\n",
    "    X9_3 = testSet.Genre_Misc.values\n",
    "    X9_4 = testSet['Genre_Role-Playing'].values\n",
    "    X9_5 = testSet.Genre_Shooter.values\n",
    "    X9_6 = testSet.Genre_Adventure.values\n",
    "    X9_7 = testSet.Genre_Racing.values\n",
    "    X9_8 = testSet.Genre_Platform.values\n",
    "    X9_9 = testSet.Genre_Simulation.values\n",
    "    X9_10 = testSet.Genre_Fighting.values\n",
    "    X9_11 = testSet.Genre_Strategy.values\n",
    "    X9_12 = testSet.Genre_Puzzle.values\n",
    "\n",
    "    X = np.array([np.ones(len(testSet.JP_Sales)),X1,X2,X3,X4,X5,X6,X7,\n",
    "                  X8_1,X8_2,X8_3,X8_4,X8_5,X8_6,X8_7,X8_8,X8_9,X8_10,X8_11,X8_12,X8_13,X8_14,X8_15,X8_16,X8_17,X8_18,X8_19,\n",
    "                  X9_1,X9_2,X9_3,X9_4,X9_5,X9_6,X9_7,X9_8,X9_9,X9_10,X9_11,X9_12]).T\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean = df_train.fillna(df_train.median()).copy()\n",
    "\n",
    "X = createX(df_train_clean)\n",
    "\n",
    "Y1 = df_train_clean.Global_Sales.values\n",
    "Y2 = df_train_clean.NA_Sales.values\n",
    "\n",
    "B1, global_data = linear_regression(Y1,X)\n",
    "B2, na_data = linear_regression(Y2,X)\n",
    "\n",
    "print(\"Global R2_prev : %.3f\" % (R2_prev(Y1,np.dot(X,B1),X)))\n",
    "print(\"NA R2_prev : %.3f\" % (R2_prev(Y2,np.dot(X,B2),X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_clean = df_test.fillna(df_train.median()).copy()\n",
    "\n",
    "X = createX(df_test_clean)\n",
    "\n",
    "test_GlobalData = np.dot(X,B1)\n",
    "test_NAData = np.dot(X,B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_estimated = pd.DataFrame([df_test.NA_Sales,df_test.Global_Sales]).copy().T\n",
    "df_test_estimated['NA_Sales'] = test_NAData\n",
    "df_test_estimated['Global_Sales'] = test_GlobalData\n",
    "df_test_estimated.to_csv(\"test_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(Y,X,varExplained):\n",
    "        \n",
    "    # On transforme X en une matrice de moyenne = 0 et variance = 1\n",
    "    scaler = StandardScaler()\n",
    "    X_std = scaler.fit_transform(X)\n",
    "    nbrOfVar = X.shape[1]\n",
    "\n",
    "    # On calcule la matrice de covariance\n",
    "    E = np.dot(X_std.T,X_std)/(nbrOfVar-1)\n",
    "\n",
    "    # On calcule nos Valeurs et Vecteurs propres\n",
    "    eig_val, eig_vec = np.linalg.eig(E)\n",
    "    eig_val = eig_val.real\n",
    "    eig_vec = eig_vec.T # pour les ordonner par range\n",
    "\n",
    "    # On calcule la contribution de chaque valeur propre a la variance total\n",
    "    eig_val_contribution = eig_val/np.sum(eig_val)\n",
    "\n",
    "    # On cree une liste d'index de 0 au nombre de valeurs propres\n",
    "    rangeIndex = np.array(range(len(eig_val_contribution)))\n",
    "\n",
    "    # On cree un tableau indexe de nos valeurs propres et leur contribution a la variance\n",
    "    eig_val_table = np.array(([rangeIndex,eig_val,eig_val_contribution])).T\n",
    "\n",
    "    # On ordonne en ordre decroissant de contribution le tableau\n",
    "    eig_val_table_ordered = eig_val_table[(-eig_val_table[:,2]).argsort()]\n",
    "    \n",
    "    # On rejoint la quantite de variance souhaitee\n",
    "    varCumul = 0.0\n",
    "    varIndex = 0\n",
    "    for val in eig_val_table_ordered:\n",
    "        varIndex += 1\n",
    "        varCumul += val[2]\n",
    "        if(varCumul >= varExplained):\n",
    "            break\n",
    "    \n",
    "    # On recupere les index des valeurs propres que nous garderont\n",
    "    indexKept = eig_val_table_ordered[:,0][:varIndex]\n",
    "\n",
    "    # Our reduced regressor\n",
    "    K_vect = eig_vec[indexKept.astype(int),:].T\n",
    "\n",
    "    # Our principal components\n",
    "    Z = np.dot(X_std,K_vect)\n",
    "\n",
    "    # Our Regression Coefficiants\n",
    "    B_partial = np.dot(np.linalg.inv(np.dot(Z.T,Z)),Z.T)\n",
    "    B = np.dot(B_partial,Y)\n",
    "\n",
    "    # We calculate the intercept\n",
    "    uY = np.mean(Y)\n",
    "\n",
    "    # We add the intercept\n",
    "    Y_new = np.dot(Z,B) + uY\n",
    "\n",
    "    # Clip to min 0\n",
    "    Y_new = Y_new.clip(min=0)\n",
    "    \n",
    "    return Y_new, B, K_vect, uY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_trained(X,B,K_vect,uY):\n",
    "    \n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Our principal components\n",
    "    Z = np.dot(X_std,K_vect)\n",
    "    \n",
    "    # We add the intercept\n",
    "    Y_new = np.dot(Z,B) + uY\n",
    "    \n",
    "    # Clip to min 0\n",
    "    Y_new = Y_new.clip(min=0)\n",
    "\n",
    "    return Y_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateCol(trainSet,testSet,colName,varExplained):\n",
    "    \n",
    "    # On definit notre variable d'interet\n",
    "    Y = np.array(trainSet[colName])\n",
    "\n",
    "    # On definit notre vecteur de variables explicatives\n",
    "    X = np.array(trainSet.drop([colName], axis=1))\n",
    "\n",
    "    # We train our model\n",
    "    [Y_new, B, K_vect, uY] = PCA(Y,X,varExplained)\n",
    "    \n",
    "    X_test = np.array(testSet.drop([colName], axis=1))    \n",
    "    Y_test = PCA_trained(X_test,B,K_vect,uY)\n",
    "    \n",
    "    return Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_estimated = pd.DataFrame([df_test.NA_Sales,df_test.Global_Sales]).copy().T\n",
    "\n",
    "\n",
    "df_test_estimated['NA_Sales'] = estimateCol(df_train,df_test,'NA_Sales',0.999)\n",
    "df_test_estimated['Global_Sales'] = estimateCol(df_train,df_test,'Global_Sales',0.999)\n",
    "\n",
    "# On definit notre vecteur de variables explicatives\n",
    "X1 = np.array(trainSet.drop(['Global_Sales'], axis=1))\n",
    "X2 = np.array(trainSet.drop(['NA_Sales'], axis=1))\n",
    "\n",
    "Y1 = df_test_estimated.Global_Sales.values\n",
    "Y2 = df_test_estimated.NA_Sales.values\n",
    "\n",
    "print(\"Global R2_prev : %.3f\" % (R2_prev(Y1,np.dot(X1,B1),X)))\n",
    "print(\"NA R2_prev : %.3f\" % (R2_prev(Y2,np.dot(X2,B2),X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
