{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "L'algorithme R2_prev implemente une validation croise sur l'ensemble du jeu de donnees fournis\n",
    "\n",
    "Arguments:\n",
    "    Y : La variable d'interet\n",
    "    W : Notre estimation de la variable d'interet\n",
    "    X : Nos variables explicatives\n",
    "    \n",
    "\n",
    "\n",
    "Retourne:\n",
    "\n",
    "    R2_prev : Une mesure bornee entre [-inf, 1] qui represente le pouvoir predictif de nos estimations\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def R2_prev(Y,W,X):\n",
    "    \n",
    "    # On multiplie X par soi meme\n",
    "    X2 = np.dot(X.T,X)  \n",
    "    \n",
    "    # The variance-covariance\n",
    "    C = np.linalg.inv(X2)\n",
    "    \n",
    "    # Hat Matrix (X*B = H*Y)\n",
    "    H = np.dot(np.dot(X,C),X.T)\n",
    "    \n",
    "    # We are only interested in the (i,i) values\n",
    "    H = np.diagonal(H)\n",
    "    \n",
    "    # Mean of Y\n",
    "    y_S = np.sum(Y)/float(len(Y))\n",
    "    \n",
    "    num = 0.0\n",
    "    for i in range(len(Y)):\n",
    "        num += ((Y[i] - W[i])/float(1 - H[i]))**2\n",
    "\n",
    "    denom = 0.0\n",
    "    for i in range(len(Y)):\n",
    "        denom += (Y[i] - y_S)**2\n",
    "    \n",
    "    R2_prev = (1 - num/denom)\n",
    "    \n",
    "    return R2_prev\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Selon le document MTH3302_CriteresProjet.pdf nous sommes evalues par notre root mean squared error (RMSE).\n",
    "On trouve ici son implementation.\n",
    "\n",
    "Arguments :\n",
    "    Y : La variable d'interet\n",
    "    W : Notre estimation de la variable d'interet\n",
    "    \n",
    "    \n",
    "Retourne:\n",
    "    rmse : L'erreur moyenne de nos predictions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def RMSE(Y,W):\n",
    "\n",
    "    n = len(Y)\n",
    "\n",
    "    total = 0.0\n",
    "    for i in range(n):\n",
    "        total += (Y[i] - W[i])**2\n",
    "        \n",
    "    rmse = total/float(n)\n",
    "        \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--- Variable Qualitative -> Table Binaire --- \n",
    "\n",
    "Afin de traiter nos variables qualitatives, nous avons une fonction qui la tranforme en table binaire \n",
    "avec un one bit encoder. \n",
    "\n",
    "    Disons que la variable explicative peut prendre 4 valeurs {Bleu, Vert, Jaune, Rouge},\n",
    "        \n",
    "          ID        X1\n",
    "        ----------------\n",
    "          1        Bleu\n",
    "          2        Bleu\n",
    "          3        Vert\n",
    "          4        Jaune\n",
    "          5        Vert\n",
    "          6        Rouge          \n",
    "         ...\n",
    "    \n",
    "    Suite a un encodage en 1 bit notre variable explicative X1 devient,\n",
    "    \n",
    "          ID        Bleu        Vert        Jaune        Rouge\n",
    "        -------------------------------------------------------\n",
    "          1          1           0            0            0\n",
    "          2          1           0            0            0\n",
    "          3          0           1            0            0\n",
    "          4          0           0            1            0\n",
    "          5          0           1            0            0\n",
    "          6          0           0            0            1\n",
    "                                ...\n",
    "                                \n",
    "\n",
    "Nous nous basons sur la fonction get_dummies() de la librairie Panda pour faire ceci.\n",
    "\n",
    "\n",
    "Arguments,\n",
    "\n",
    "    trainSet : le jeu de donnees de training\n",
    "    testSet : le jeu de donnees de test\n",
    "    varName : la variable qualitative que nous souhaitons transformer en tableau binaire\n",
    "    \n",
    "    \n",
    "Retourne, \n",
    "    trainSet_new : le jeu de donnees de training ou la variable qualitative a ete remplacee par un tableau binaire\n",
    "    testSet_new : le jeu de donnees de test ou la variable qualitative a ete remplacee par un tableau binaire\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def qualiToBinary(trainSet, testSet, varName):\n",
    "    \n",
    "    # Genere la table binaire de la variable qualitative\n",
    "    trainVar_binary = pd.get_dummies(trainSet[varName],prefix=varName)\n",
    "    testVar_binary = pd.get_dummies(testSet[varName],prefix=varName)\n",
    "    \n",
    "    # On convertit nos tableaux binaires en Dataframe\n",
    "    trainVar_binary = pd.DataFrame(trainVar_binary)\n",
    "    testVar_binary = pd.DataFrame(testVar_binary)\n",
    "    \n",
    "    # On enleve la variable qualitative de nos jeux de donnees\n",
    "    trainSet_new = trainSet.drop([varName], axis=1)\n",
    "    testSet_new = testSet.drop([varName], axis=1)   \n",
    "\n",
    "    # On ajoute les nouvelles colonnes a nos jeux de donnees\n",
    "    trainSet_new = pd.concat([trainSet_new,trainVar_binary],axis=1)\n",
    "    testSet_new = pd.concat([testSet_new,testVar_binary],axis=1)\n",
    "    \n",
    "    # On identifie les colonnes manquantes dans le jeu de donnees test\n",
    "    missing_categories = set(trainSet_new) - set(testSet_new)\n",
    "\n",
    "    # On ajoute ces colonnes manquantes avec une valeur par defaut de 0\n",
    "    for c in missing_categories:\n",
    "        testSet_new[c] = 0\n",
    "\n",
    "    # On identifie les colonnes manquantes dans le jeu de donnees training\n",
    "    missing_categories = set(testSet_new) - set(trainSet_new)\n",
    "\n",
    "    # On ajoute ces colonnes manquantes avec une valeur par defaut de 0\n",
    "    for c in missing_categories:\n",
    "        trainSet_new[c] = 0\n",
    "\n",
    "\n",
    "    # On aligne nos jeux de donnees pour que les memes colonnes soient aux memes endroits\n",
    "    trainSet_new, testSet_new = trainSet_new.align(testSet_new, axis=1)\n",
    "    \n",
    "    # On retourne les jeux de donnees\n",
    "    return trainSet_new, testSet_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(Y,X,varExplained):\n",
    "        \n",
    "    # On transforme X en une matrice de moyenne = 0 et variance = 1\n",
    "    scaler = StandardScaler()\n",
    "    X_std = scaler.fit_transform(X)\n",
    "    nbrOfVar = X.shape[1]\n",
    "\n",
    "    # On calcule la matrice de covariance\n",
    "    E = np.dot(X_std.T,X_std)/(nbrOfVar-1)\n",
    "\n",
    "    # On calcule nos Valeurs et Vecteurs propres\n",
    "    eig_val, eig_vec = np.linalg.eig(E)\n",
    "    eig_val = eig_val.real\n",
    "    eig_vec = eig_vec.real\n",
    "    eig_vec = eig_vec.T # pour les ordonner par range\n",
    "\n",
    "    # On calcule la contribution de chaque valeur propre a la variance total\n",
    "    eig_val_contribution = eig_val/np.sum(eig_val)\n",
    "\n",
    "    # On cree une liste d'index de 0 au nombre de valeurs propres\n",
    "    rangeIndex = np.array(range(len(eig_val_contribution)))\n",
    "\n",
    "    # On cree un tableau indexe de nos valeurs propres et leur contribution a la variance\n",
    "    eig_val_table = np.array(([rangeIndex,eig_val,eig_val_contribution])).T\n",
    "\n",
    "    # On ordonne en ordre decroissant de contribution le tableau\n",
    "    eig_val_table_ordered = eig_val_table[(-eig_val_table[:,2]).argsort()]\n",
    "    \n",
    "    # On rejoint la quantite de variance souhaitee\n",
    "    varCumul = 0.0\n",
    "    varIndex = 0\n",
    "    for val in eig_val_table_ordered:\n",
    "        varIndex += 1\n",
    "        varCumul += val[2]\n",
    "        if(varCumul >= varExplained):\n",
    "            break\n",
    "    \n",
    "    print(\"Nbr of Var = %d\" % varIndex)\n",
    "    \n",
    "    # On recupere les index des valeurs propres que nous garderont\n",
    "    indexKept = eig_val_table_ordered[:,0][:varIndex]\n",
    "\n",
    "    # Our reduced regressor\n",
    "    K_vect = eig_vec[indexKept.astype(int),:].T\n",
    "\n",
    "    # Our principal components\n",
    "    Z = np.dot(X_std,K_vect)\n",
    "\n",
    "    # Our Regression Coefficiants\n",
    "    B_partial = np.dot(np.linalg.inv(np.dot(Z.T,Z)),Z.T)\n",
    "    B = np.dot(B_partial,Y)\n",
    "\n",
    "    # We calculate the intercept\n",
    "    uY = np.mean(Y)\n",
    "\n",
    "    # We add the intercept\n",
    "    Y_new = np.dot(Z,B) + uY\n",
    "\n",
    "    # Clip to min 0\n",
    "    Y_new = Y_new.clip(min=0)\n",
    "    \n",
    "    return Y_new, B, K_vect, uY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_trained(X,B,K_vect,uY):\n",
    "    \n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Our principal components\n",
    "    Z = np.dot(X_std,K_vect)\n",
    "    \n",
    "    # We add the intercept\n",
    "    Y_new = np.dot(Z,B) + uY\n",
    "    \n",
    "    # Clip to min 0\n",
    "    Y_new = Y_new.clip(min=0)\n",
    "\n",
    "    return Y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe les librairies\n",
    "import numpy as np # calcul matriciel\n",
    "import pandas as pd # structure des donnees\n",
    "import matplotlib.pyplot as plt # graphiques\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Chemin vers les jeux de donnees en format csv\n",
    "pathTest = \"./test3.csv\"\n",
    "pathTrain = \"./train3.csv\"\n",
    "\n",
    "# On importe les jeux de donnees\n",
    "df_test = pd.read_csv(pathTest, sep=',', index_col=0)\n",
    "df_train = pd.read_csv(pathTrain, sep=',', index_col=0)\n",
    "\n",
    "# On enleve la colonne Name qui ne sert pas a l'analyse\n",
    "df_train = df_train.drop(['Name'],axis=1)\n",
    "\n",
    "# --- Variables Explicatives ---\n",
    "\n",
    "#    Name : Nom du jeu\n",
    "#    Platform : Console sur laquelle le jeu fonctionne\n",
    "#    Year_of_Release : Année de sortie du jeu\n",
    "#    Genre\n",
    "#    Publisher\n",
    "#    JP_Sales : Nombre de ventes du jeu au Japon en millions d’unités\n",
    "#    Other_Sales : Nombre de ventes du jeu ailleurs dans le monde : Afrique, Asie sans le Japon, Europe sans l’Union Européenne et Amérique du Sud en millions d’unités\n",
    "#    Critic_Score : Score donné par Metacritic\n",
    "#    Critic_Count : Nombre de critiques prises en compte pour estimer le Critic_score\n",
    "#    User_Score : Score donné par les usagers de Metacritic\n",
    "#    User_Count : Nombre d’usagers considérés pour estimer le User_Score\n",
    "#    Developer : Compagnie créatrice du jeu\n",
    "#    Rating : Classement ESRB (Entertainment Software Rating Board) ie à qui s’addresse le jeu (tout public, majeur, adolescents, etc) \n",
    "\n",
    "\n",
    "# --- Variables d'Interet ---\n",
    "\n",
    "#    NA_sales : Nombre de ventes du jeu en Amérique du Nord en millions d’unités\n",
    "#    Global_Sales : Nombre de ventes total du jeu en millions d’unités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On donne une moyenne des Critic et User scores\n",
    "df_train['Score'] = (df_train.Critic_Score + df_train.User_Score*10)/2\n",
    "df_test['Score'] = (df_test.Critic_Score + df_test.User_Score*10)/2\n",
    "\n",
    "#L'âge de la plateforme à la sortie du jeu\n",
    "df_train['Platform_Age'] = df_train.Year_of_Release - df_train.groupby(['Platform'])['Year_of_Release'].transform(min)\n",
    "df_test['Platform_Age'] = df_test.Year_of_Release - df_test.groupby(['Platform'])['Year_of_Release'].transform(min)\n",
    "\n",
    "#La moyenne des ventes par plateforme et genre\n",
    "df_train['Mean_Sales_by_Platform'] = df_train.groupby(['Platform'])['Other_Sales'].transform('mean')\n",
    "df_train['Mean_Sales_by_Genre'] = df_train.groupby(['Genre'])['Other_Sales'].transform('mean')\n",
    "df_test['Mean_Sales_by_Platform'] = df_test.groupby(['Platform'])['Other_Sales'].transform('mean')\n",
    "df_test['Mean_Sales_by_Genre'] = df_test.groupby(['Genre'])['Other_Sales'].transform('mean')\n",
    "\n",
    "#Le nombre de jeux sortis la même année\n",
    "df_train['Num_Games_Same_Year'] = df_train.groupby(['Year_of_Release'])['Year_of_Release'].transform('count')\n",
    "df_test['Num_Games_Same_Year'] = df_test.groupby(['Year_of_Release'])['Year_of_Release'].transform('count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr. of categories Platform (train) : 30\n",
      "Nbr. of categories Platform (test) : 27 \n",
      "\n",
      "Nbr. of categories Genre (train) : 12\n",
      "Nbr. of categories Genre (test) : 12 \n",
      "\n",
      "Nbr. of categories Publisher (train) : 552\n",
      "Nbr. of categories Publisher (test) : 280 \n",
      "\n",
      "Nbr. of categories Developer (train) : 1593\n",
      "Nbr. of categories Developer (test) : 677 \n",
      "\n",
      "Nbr. of categories Rating (train) : 8\n",
      "Nbr. of categories Rating (test) : 5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Ici on transforme nos variables qualitatives en tableau binaire\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Variable Qualitative #1 : Platform\n",
    "print(\"Nbr. of categories Platform (train) : %d\" %(df_train.Platform.nunique()))\n",
    "print(\"Nbr. of categories Platform (test) : %d \\n\" %(df_test.Platform.nunique()))\n",
    "df_train, df_test = qualiToBinary(df_train, df_test, 'Platform') # On binairise\n",
    "\n",
    "\n",
    "# Variable Qualitative #2 : Genre\n",
    "print(\"Nbr. of categories Genre (train) : %d\" %(df_train.Genre.nunique()))\n",
    "print(\"Nbr. of categories Genre (test) : %d \\n\" %(df_test.Genre.nunique()))\n",
    "df_train, df_test = qualiToBinary(df_train, df_test,'Genre') # On binairise\n",
    "\n",
    "\n",
    "# Variable Qualitative #3 : Publisher\n",
    "print(\"Nbr. of categories Publisher (train) : %d\" %(df_train.Publisher.nunique()))\n",
    "print(\"Nbr. of categories Publisher (test) : %d \\n\" %(df_test.Publisher.nunique()))\n",
    "df_train, df_test = qualiToBinary(df_train, df_test, 'Publisher') # On binairise\n",
    "#df_train = df_train.drop(['Publisher'], axis=1)\n",
    "#df_test = df_test.drop(['Publisher'], axis=1)\n",
    "\n",
    "# Variable Qualitative #4 : Developer\n",
    "print(\"Nbr. of categories Developer (train) : %d\" %(df_train.Developer.nunique()))\n",
    "print(\"Nbr. of categories Developer (test) : %d \\n\" %(df_test.Developer.nunique()))\n",
    "#df_train, df_test = qualiToBinary(df_train, df_test,'Developer') # On binairise\n",
    "df_train = df_train.drop(['Developer'], axis=1)\n",
    "df_test = df_test.drop(['Developer'], axis=1)\n",
    "\n",
    "# Variable Qualitative #5 : Rating\n",
    "print(\"Nbr. of categories Rating (train) : %d\" %(df_train.Rating.nunique()))\n",
    "print(\"Nbr. of categories Rating (test) : %d \\n\" %(df_test.Rating.nunique()))\n",
    "df_train, df_test = qualiToBinary(df_train, df_test,'Rating') # On binairise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Ici on ajoute de nouvelles colonnes qui sont des combinaisons non-lineaires \n",
    "\n",
    "des autres colonnes ou de toute nouvelles informations.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# On ajoute une colonne qui comptabilise le nombre de NaN pour chaque jeu video\n",
    "nbrOfNaN_train = ~(df_train.T.iloc[:].isna().sum().astype(bool))\n",
    "df_train['Contains_NaN'] = nbrOfNaN_train.astype(np.uint8)\n",
    "\n",
    "nbrOfNaN_test = ~(df_test.T.iloc[:].isna().sum().astype(bool))\n",
    "df_test['Contains_NaN'] = nbrOfNaN_test.astype(np.uint8)\n",
    "\n",
    "df_train['JP_Sales2'] = df_train.JP_Sales**2\n",
    "df_test['JP_Sales2'] = df_test.JP_Sales**2\n",
    "\n",
    "df_train['Other_Sales2'] = df_train.Other_Sales**2\n",
    "df_test['Other_Sales2'] = df_test.Other_Sales**2\n",
    "\n",
    "df_train['JP_Sales_Other_Sales'] = df_train.Other_Sales*df_train.JP_Sales\n",
    "df_test['JP_Sales_Other_Sales'] = df_test.Other_Sales*df_test.JP_Sales\n",
    "\n",
    "df_train['Other_Sales_Year_of_Release'] = df_train.Other_Sales*df_train.Year_of_Release\n",
    "df_test['Other_Sales_Year_of_Release'] = df_test.Other_Sales*df_test.Year_of_Release\n",
    "\n",
    "df_train['JP_Sales_Year_of_Release'] = df_train.JP_Sales*df_train.Year_of_Release\n",
    "df_test['JP_Sales_Year_of_Release'] = df_test.JP_Sales*df_test.Year_of_Release\n",
    "\n",
    "df_train['JP_Sales_Year2_of_Release2'] = df_train.JP_Sales**2*df_train.Year_of_Release**2\n",
    "df_test['JP_Sales_Year2_of_Release2'] = df_test.JP_Sales**2*df_test.Year_of_Release**2\n",
    "\n",
    "df_train['Critic_Score9_Critic_Count'] = df_train.Critic_Score**9*df_train.Critic_Count\n",
    "df_test['Critic_Score9_Critic_Count'] = df_test.Critic_Score**9*df_test.Critic_Count\n",
    "\n",
    "df_train['Critic_Score12_Critic_Count'] = df_train.Critic_Score**12*df_train.Critic_Count\n",
    "df_test['Critic_Score12_Critic_Count'] = df_test.Critic_Score**12*df_test.Critic_Count\n",
    "\n",
    "df_train['Critic_Score_Critic_Count'] = df_train.Critic_Score*df_train.Critic_Count\n",
    "df_test['Critic_Score_Critic_Count'] = df_test.Critic_Score*df_test.Critic_Count\n",
    "\n",
    "df_train['User_Score_User_Count'] = df_train.User_Score*df_train.User_Count\n",
    "df_test['User_Score_User_Count'] = df_test.User_Score*df_test.User_Count\n",
    "\n",
    "df_train['User_Score2'] = df_train.User_Score**2\n",
    "df_test['User_Score2'] = df_test.User_Score**2\n",
    "\n",
    "df_train['User_Count2'] = df_train.User_Count**2\n",
    "df_test['User_Count2'] = df_test.User_Count**2\n",
    "\n",
    "df_train['Critic_Count2'] = df_train.Critic_Count**2\n",
    "df_test['Critic_Count2'] = df_test.Critic_Count**2\n",
    "\n",
    "df_train['Critic_Score2'] = df_train.Critic_Score**2\n",
    "df_test['Critic_Score2'] = df_test.Critic_Score**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean = df_train.fillna(df_train.median())\n",
    "df_test_clean = df_test.fillna(df_train.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On fait une matrice de nos variables\n",
    "X_train = df_train_clean.drop(['NA_Sales','Global_Sales'],axis=1)\n",
    "X_test = df_test_clean.drop(['NA_Sales','Global_Sales'],axis=1)\n",
    "\n",
    "# On isole nos variables d'interet\n",
    "Y1 = df_train_clean.Global_Sales.values\n",
    "Y2 = df_train_clean.NA_Sales.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr of Var = 624\n"
     ]
    }
   ],
   "source": [
    "Y_new1, B1, K_vect1, uY1 = PCA(Y1,X_train,0.999999)\n",
    "rmse1 = RMSE(Y1,Y_new1)\n",
    "\n",
    "Y1_test = PCA_trained(X_test,B1, K_vect1, uY1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbr of Var = 624\n"
     ]
    }
   ],
   "source": [
    "Y_new2, B2, K_vect2, uY2 = PCA(Y2,X_train,0.999999)\n",
    "rmse2 = RMSE(Y2,Y_new2)\n",
    "\n",
    "Y2_test = PCA_trained(X_test,B2, K_vect2, uY2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.4432\n"
     ]
    }
   ],
   "source": [
    "rmseTot = np.sqrt(rmse1**2 + rmse2**2)\n",
    "print(\"RMSE = %.4f\" % rmseTot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Processing du jeu de donnees test\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# On vient prendre le dataframe comme canva\n",
    "df_test_estimated = pd.DataFrame([df_test.NA_Sales,df_test.Global_Sales]).copy().T\n",
    "\n",
    "# On assigne nos predictions aux colonnes\n",
    "df_test_estimated['Global_Sales'] = Y1_test\n",
    "df_test_estimated['NA_Sales'] = Y2_test\n",
    "\n",
    "# On sauvegarde nos predictions\n",
    "df_test_estimated.to_csv(\"test_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
